<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hadoop常用命令</title>
    <link href="/2020/11/29/Hdfs%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/11/29/Hdfs%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h3 id="hadoop命令"><a href="#hadoop命令" class="headerlink" title="hadoop命令"></a>hadoop命令</h3><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ bin/hadoop Usage: hadoop [--config confdir] COMMAND       where COMMAND <span class="hljs-keyword">is</span> one of:  fs                   run a generic filesystem user client  version              print the version  jar &lt;jar&gt;            run a jar file  checknative [-a|-h]  check native hadoop <span class="hljs-keyword">and</span> compression libraries availability  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file <span class="hljs-keyword">or</span> directories recursively  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive  classpath            prints the <span class="hljs-keyword">class</span> <span class="hljs-symbol">path</span> <span class="hljs-symbol">needed</span> <span class="hljs-symbol">to</span> <span class="hljs-symbol">get</span> <span class="hljs-symbol">the</span>  <span class="hljs-symbol">credential</span>           <span class="hljs-symbol">interact</span> <span class="hljs-symbol">with</span> <span class="hljs-symbol">credential</span> <span class="hljs-symbol">providers</span>                       <span class="hljs-symbol">Hadoop</span> <span class="hljs-symbol">jar</span> <span class="hljs-symbol">and</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">required</span> <span class="hljs-symbol">libraries</span>  <span class="hljs-symbol">daemonlog</span>            <span class="hljs-symbol">get</span>/<span class="hljs-symbol">set</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">log</span> <span class="hljs-symbol">level</span> <span class="hljs-symbol">for</span> <span class="hljs-symbol">each</span> <span class="hljs-symbol">daemon</span>  <span class="hljs-symbol">s3guard</span>              <span class="hljs-symbol">manage</span> <span class="hljs-symbol">data</span> <span class="hljs-symbol">on</span> <span class="hljs-symbol">S3</span>  <span class="hljs-symbol">trace</span>                <span class="hljs-symbol">view</span> <span class="hljs-symbol">and</span> <span class="hljs-symbol">modify</span> <span class="hljs-symbol">Hadoop</span> <span class="hljs-symbol">tracing</span> <span class="hljs-symbol">settings</span> <span class="hljs-symbol">or</span>  <span class="hljs-symbol">CLASSNAME</span>            <span class="hljs-symbol">run</span> <span class="hljs-symbol">the</span> <span class="hljs-symbol">class</span> <span class="hljs-symbol">named</span> <span class="hljs-symbol">CLASSNAME</span></code></pre><h4 id="hadoop-checknative是否支持压缩"><a href="#hadoop-checknative是否支持压缩" class="headerlink" title="hadoop checknative是否支持压缩"></a>hadoop checknative是否支持压缩</h4><pre><code class="hljs yaml">[<span class="hljs-string">hadoop@hadoop100</span> <span class="hljs-string">hadoop</span>]<span class="hljs-string">$</span> <span class="hljs-string">hadoop</span> <span class="hljs-string">checknative</span><span class="hljs-attr">Native library checking:</span><span class="hljs-attr">hadoop:</span>  <span class="hljs-literal">false</span> <span class="hljs-attr">zlib:</span>    <span class="hljs-literal">false</span> <span class="hljs-attr">snappy:</span>  <span class="hljs-literal">false</span> <span class="hljs-attr">lz4:</span>     <span class="hljs-literal">false</span> <span class="hljs-attr">bzip2:</span>   <span class="hljs-literal">false</span> <span class="hljs-attr">openssl:</span> <span class="hljs-literal">false</span></code></pre><h3 id="hdfs命令"><a href="#hdfs命令" class="headerlink" title="hdfs命令"></a>hdfs命令</h3><pre><code class="hljs routeros">[hadoop@hadoop100 hadoop]$ bin/hdfs Usage: hdfs [--config confdir] COMMAND       where COMMAND is one of:  dfs                  <span class="hljs-builtin-name">run</span> a filesystem command on the file systems supported <span class="hljs-keyword">in</span> Hadoop.  namenode -format     format the DFS filesystem  secondarynamenode    <span class="hljs-builtin-name">run</span> the DFS secondary namenode  namenode             <span class="hljs-builtin-name">run</span> the DFS namenode  journalnode          <span class="hljs-builtin-name">run</span> the DFS journalnode  zkfc                 <span class="hljs-builtin-name">run</span> the ZK Failover Controller daemon  datanode             <span class="hljs-builtin-name">run</span> a DFS datanode  dfsadmin             <span class="hljs-builtin-name">run</span> a DFS admin client  diskbalancer         Distributes data evenly among disks on a given node  haadmin              <span class="hljs-builtin-name">run</span> a DFS HA admin client  fsck                 <span class="hljs-builtin-name">run</span> a DFS filesystem checking utility  balancer             <span class="hljs-builtin-name">run</span> a cluster balancing utility  jmxget               <span class="hljs-builtin-name">get</span> JMX exported values <span class="hljs-keyword">from</span> NameNode <span class="hljs-keyword">or</span> DataNode.  mover                <span class="hljs-builtin-name">run</span> a utility <span class="hljs-keyword">to</span> move block replicas across                       storage types  oiv                  apply the offline fsimage viewer <span class="hljs-keyword">to</span> an fsimage  oiv_legacy           apply the offline fsimage viewer <span class="hljs-keyword">to</span> an legacy fsimage  oev                  apply the offline edits viewer <span class="hljs-keyword">to</span> an edits file  fetchdt              fetch a delegation token <span class="hljs-keyword">from</span> the NameNode  getconf              <span class="hljs-builtin-name">get</span><span class="hljs-built_in"> config </span>values <span class="hljs-keyword">from</span> configuration  groups               <span class="hljs-builtin-name">get</span> the groups which<span class="hljs-built_in"> users </span>belong <span class="hljs-keyword">to</span>  snapshotDiff         diff two snapshots of a directory <span class="hljs-keyword">or</span> diff the                       current directory contents with a snapshot  lsSnapshottableDir   list all snapshottable dirs owned by the current userUse -help <span class="hljs-keyword">to</span> see options  portmap              <span class="hljs-builtin-name">run</span> a portmap service  nfs3                 <span class="hljs-builtin-name">run</span> an NFS version 3 gateway  cacheadmin           configure the HDFS cache  crypto               configure HDFS encryption zones  storagepolicies      list/get/<span class="hljs-builtin-name">set</span> block storage policies  version              <span class="hljs-builtin-name">print</span> the version</code></pre><h4 id="hdfs-dfs"><a href="#hdfs-dfs" class="headerlink" title="hdfs dfs"></a>hdfs dfs</h4><ul><li>hdfs dfs 和 hadoop fs是等价的 <pre><code class="hljs routeros">查看hadoop命令文件<span class="hljs-comment"># the core commands</span> <span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$COMMAND</span>&quot;</span> = <span class="hljs-string">&quot;fs&quot;</span> ] ; then   <span class="hljs-attribute">CLASS</span>=org.apache.hadoop.fs.FsShell查看hdfs命令文件elif [ <span class="hljs-string">&quot;<span class="hljs-variable">$COMMAND</span>&quot;</span> = <span class="hljs-string">&quot;dfs&quot;</span> ] ; then   <span class="hljs-attribute">CLASS</span>=org.apache.hadoop.fs.FsShell</code></pre></li><li>hdfs dfs 命令中常用的参数<pre><code class="hljs inform7"> <span class="hljs-comment">[hadoop@hadoop100 bin]</span>$ hdfs dfs Usage: hadoop fs <span class="hljs-comment">[generic options]</span><span class="hljs-comment">[-cat <span class="hljs-comment">[-ignoreCrc]</span> &lt;src&gt; ...]</span> 【查看文件内容】<span class="hljs-comment">[-chmod <span class="hljs-comment">[-R]</span> &lt;MODE<span class="hljs-comment">[,MODE]</span>... | OCTALMODE&gt; PATH...]</span><span class="hljs-comment">[-chown <span class="hljs-comment">[-R]</span> <span class="hljs-comment">[OWNER]</span><span class="hljs-comment">[:<span class="hljs-comment">[GROUP]</span>]</span> PATH...]</span><span class="hljs-comment">[-copyFromLocal <span class="hljs-comment">[-f]</span> <span class="hljs-comment">[-p]</span> <span class="hljs-comment">[-l]</span> &lt;localsrc&gt; ... &lt;dst&gt;]</span> 【等价于put】<span class="hljs-comment">[-copyToLocal <span class="hljs-comment">[-p]</span> <span class="hljs-comment">[-ignoreCrc]</span> <span class="hljs-comment">[-crc]</span> &lt;src&gt; ... &lt;localdst&gt;]</span> 【等价于get】<span class="hljs-comment">[-cp <span class="hljs-comment">[-f]</span> <span class="hljs-comment">[-p | -p<span class="hljs-comment">[topax]</span>]</span> &lt;src&gt; ... &lt;dst&gt;]</span><span class="hljs-comment">[-du <span class="hljs-comment">[-s]</span> <span class="hljs-comment">[-h]</span> <span class="hljs-comment">[-x]</span> &lt;path&gt; ...]</span><span class="hljs-comment">[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><span class="hljs-comment">[-get <span class="hljs-comment">[-p]</span> <span class="hljs-comment">[-ignoreCrc]</span> <span class="hljs-comment">[-crc]</span> &lt;src&gt; ... &lt;localdst&gt;]</span><span class="hljs-comment">[-help <span class="hljs-comment">[cmd ...]</span>]</span><span class="hljs-comment">[-ls <span class="hljs-comment">[-C]</span> <span class="hljs-comment">[-d]</span> <span class="hljs-comment">[-h]</span> <span class="hljs-comment">[-q]</span> <span class="hljs-comment">[-R]</span> <span class="hljs-comment">[-t]</span> <span class="hljs-comment">[-S]</span> <span class="hljs-comment">[-r]</span> <span class="hljs-comment">[-u]</span> <span class="hljs-comment">[&lt;path&gt; ...]</span>]</span><span class="hljs-comment">[-mkdir <span class="hljs-comment">[-p]</span> &lt;path&gt; ...]</span><span class="hljs-comment">[-mv &lt;src&gt; ... &lt;dst&gt;]</span> 【生产上不建议使用，如果移动中有问题会导致数据不全。建议先使用cp，如果没问题再删除源端】<span class="hljs-comment">[-put <span class="hljs-comment">[-f]</span> <span class="hljs-comment">[-p]</span> <span class="hljs-comment">[-l]</span> &lt;localsrc&gt; ... &lt;dst&gt;]</span><span class="hljs-comment">[-rm <span class="hljs-comment">[-f]</span> <span class="hljs-comment">[-r|-R]</span> <span class="hljs-comment">[-skipTrash]</span> &lt;src&gt; ...]</span>  【skipTrash 不建议使用】<span class="hljs-comment">[-rmdir <span class="hljs-comment">[--ignore-fail-on-non-empty]</span> &lt;dir&gt; ...]</span></code></pre><h4 id="hdfs-dfsadmin-【dfs管理操作命令】"><a href="#hdfs-dfsadmin-【dfs管理操作命令】" class="headerlink" title="hdfs dfsadmin 【dfs管理操作命令】"></a>hdfs dfsadmin 【dfs管理操作命令】</h4><pre><code class="hljs prolog">[hadoop@hadoop100 bin]$ hdfs dfsadmin<span class="hljs-symbol">Usage</span>: hdfs dfsadmin<span class="hljs-symbol">Note</span>: <span class="hljs-symbol">Administrative</span> commands can only be run as the <span class="hljs-symbol">HDFS</span> superuser.[-report [-live] [-dead] [-decommissioning]][-safemode &lt;enter | leave | get | wait&gt;][-saveNamespace][-rollEdits][-restoreFailedStorage true|false|check][-refreshNodes][-setQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;][-clrQuota &lt;dirname&gt;...&lt;dirname&gt;][-setSpaceQuota &lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;][-clrSpaceQuota &lt;dirname&gt;...&lt;dirname&gt;][-finalizeUpgrade][-rollingUpgrade [&lt;query|prepare|finalize&gt;]][-refreshServiceAcl][-refreshUserToGroupsMappings][-refreshSuperUserGroupsConfiguration][-refreshCallQueue][-refresh &lt;host:ipc_port&gt; &lt;key&gt; [arg1..argn][-reconfig &lt;datanode|...&gt; &lt;host:ipc_port&gt; &lt;start|status|properties&gt;][-printTopology][-refreshNamenodes datanode_host:ipc_port][-deleteBlockPool datanode_host:ipc_port blockpoolId [force]][-setBalancerBandwidth &lt;bandwidth in bytes per second&gt;][-fetchImage &lt;local directory&gt;][-allowSnapshot &lt;snapshotDir&gt;][-disallowSnapshot &lt;snapshotDir&gt;][-shutdownDatanode &lt;datanode_host:ipc_port&gt; [upgrade]][-getDatanodeInfo &lt;datanode_host:ipc_port&gt;][-metasave filename][-triggerBlockReport [-incremental] &lt;datanode_host:ipc_port&gt;][-listOpenFiles [-blockingDecommission] [-path &lt;path&gt;]][-help [cmd]]</code></pre><h5 id="hdfs-dfsadmin-safemode-安全模式"><a href="#hdfs-dfsadmin-safemode-安全模式" class="headerlink" title="hdfs dfsadmin -safemode 安全模式"></a>hdfs dfsadmin -safemode 安全模式</h5></li><li>查看、进入、离开安全模式：hdfs dfs admin -safemode get/enter/leave<pre><code class="hljs csharp">[<span class="hljs-meta">hadoop@hadoop100 bin</span>]$ hdfs dfsadmin -safemode Usage: hdfs dfsadmin [-safemode enter | leave | <span class="hljs-keyword">get</span>   | wait][<span class="hljs-meta">hadoop@hadoop100 bin</span>]$ hdfs dfsadmin -safemode <span class="hljs-keyword">get</span> Safe mode <span class="hljs-keyword">is</span> OFF</code></pre></li><li>OFF离开安全模式，客户端可以对hdfs集群进行读写</li><li>ON进入安全模式，客户端对hdfs集群可以读，但是不能写入<br>不可以写<pre><code class="hljs vim">[hadoop@hadoop100 bin]$ hdfs dfsadmin -safemode enterSafe <span class="hljs-keyword">mode</span> <span class="hljs-keyword">is</span> ON [hadoop@hadoop100 bin]$ hadoop fs -<span class="hljs-keyword">put</span> yarn /<span class="hljs-keyword">pu</span><span class="hljs-variable">t:</span> Cannot create <span class="hljs-keyword">file</span>/yarn._COPYING_. Name node <span class="hljs-keyword">is</span> in safe <span class="hljs-keyword">mode</span>.</code></pre>可以查看<pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 bin]</span>$ hadoop fs -ls /-rwxr-xr-x   <span class="hljs-number">1</span> hadoop supergroup          <span class="hljs-number">4</span> <span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-26</span> <span class="hljs-number">17</span>:<span class="hljs-number">07</span> /a.log-rw-r--r--   <span class="hljs-number">1</span> hadoop supergroup         <span class="hljs-number">11</span> <span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-22</span> <span class="hljs-number">14</span>:<span class="hljs-number">29</span> /inputdrwxr-xr-x   - hadoop supergroup          <span class="hljs-number">0</span> <span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-22</span> <span class="hljs-number">14</span>:<span class="hljs-number">30</span> /output</code></pre></li><li>集群进入安全模式可能有以下情况<br>丢失块，达不到阈值集群自动会进入安全模式<br>业务需要进行维护，手动进入安全模式<br>hdfs集群重新启动刚开始会进入安全模式<h4 id="hdfs-fsck-块的健康信息检查"><a href="#hdfs-fsck-块的健康信息检查" class="headerlink" title="hdfs fsck 块的健康信息检查"></a>hdfs fsck 块的健康信息检查</h4><pre><code class="hljs yaml">[<span class="hljs-string">hadoop@hadoop100</span> <span class="hljs-string">bin</span>]<span class="hljs-string">$</span> <span class="hljs-string">hdfs</span> <span class="hljs-string">fsck</span> <span class="hljs-string">/</span><span class="hljs-attr">20/11/29 14:30:32 WARN util.NativeCodeLoader:</span> <span class="hljs-string">Unable</span> <span class="hljs-string">to</span> <span class="hljs-string">load</span> <span class="hljs-string">native-hadoop</span> <span class="hljs-string">library</span> <span class="hljs-string">for</span> <span class="hljs-string">your</span> <span class="hljs-string">platform...</span> <span class="hljs-string">using</span> <span class="hljs-string">builtin-java</span> <span class="hljs-string">classes</span> <span class="hljs-string">where</span> <span class="hljs-string">applicable</span><span class="hljs-string">Connecting</span> <span class="hljs-string">to</span> <span class="hljs-string">namenode</span> <span class="hljs-string">via</span> <span class="hljs-string">http://hadoop100:50070/fsck?ugi=hadoop&amp;path=%2F</span><span class="hljs-string">FSCK</span> <span class="hljs-string">started</span> <span class="hljs-string">by</span> <span class="hljs-string">hadoop</span> <span class="hljs-string">(auth:SIMPLE)</span> <span class="hljs-string">from</span> <span class="hljs-string">/192.168.1.100</span> <span class="hljs-string">for</span> <span class="hljs-string">path</span> <span class="hljs-string">/</span> <span class="hljs-string">at</span> <span class="hljs-string">Sun</span> <span class="hljs-string">Nov</span> <span class="hljs-number">29</span> <span class="hljs-number">14</span><span class="hljs-string">:30:34</span> <span class="hljs-string">CST</span> <span class="hljs-number">2020</span><span class="hljs-string">.</span><span class="hljs-string">/a.log:</span> <span class="hljs-string">CORRUPT</span> <span class="hljs-string">blockpool</span> <span class="hljs-string">BP-1678315190-192.168.1.100-1606024642916</span> <span class="hljs-string">block</span> <span class="hljs-string">blk_1073741858</span><span class="hljs-string">/a.log:</span> <span class="hljs-attr">MISSING 1 blocks of total size 4 B.....................Status:</span> <span class="hljs-string">CORRUPT</span> <span class="hljs-attr">Total size:</span><span class="hljs-number">138189</span> <span class="hljs-string">B</span> <span class="hljs-attr">Total dirs:</span><span class="hljs-number">184</span> <span class="hljs-attr">Total files:</span><span class="hljs-number">21</span> <span class="hljs-attr">Total symlinks:</span><span class="hljs-number">0</span> <span class="hljs-string">Total</span> <span class="hljs-string">blocks</span> <span class="hljs-string">(validated):</span><span class="hljs-number">20</span> <span class="hljs-string">(avg.</span> <span class="hljs-string">block</span> <span class="hljs-string">size</span> <span class="hljs-number">6909 </span><span class="hljs-string">B)</span>  <span class="hljs-string">********************************</span>  <span class="hljs-string">UNDER</span> <span class="hljs-string">MIN</span> <span class="hljs-string">REPL&#x27;D</span> <span class="hljs-attr">BLOCKS:</span><span class="hljs-number">1</span> <span class="hljs-string">(5.0</span> <span class="hljs-string">%)</span>  <span class="hljs-attr">dfs.namenode.replication.min:</span><span class="hljs-number">1</span>  <span class="hljs-attr">CORRUPT FILES:</span><span class="hljs-number">1</span>  <span class="hljs-attr">MISSING BLOCKS:</span><span class="hljs-number">1</span>  <span class="hljs-attr">MISSING SIZE:</span><span class="hljs-number">4</span> <span class="hljs-string">B</span>  <span class="hljs-attr">CORRUPT BLOCKS:</span> <span class="hljs-number">1</span>  <span class="hljs-string">********************************</span> <span class="hljs-attr">Minimally replicated blocks:</span><span class="hljs-number">19</span> <span class="hljs-string">(95.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Over-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Under-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Mis-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Default replication factor:</span><span class="hljs-number">1</span> <span class="hljs-attr">Average block replication:</span><span class="hljs-number">0.95</span> <span class="hljs-attr">Corrupt blocks:</span><span class="hljs-number">1</span> <span class="hljs-attr">Missing replicas:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Number of data-nodes:</span><span class="hljs-number">1</span> <span class="hljs-attr">Number of racks:</span><span class="hljs-number">1</span><span class="hljs-string">FSCK</span> <span class="hljs-string">ended</span> <span class="hljs-string">at</span> <span class="hljs-string">Sun</span> <span class="hljs-string">Nov</span> <span class="hljs-number">29</span> <span class="hljs-number">14</span><span class="hljs-string">:30:34</span> <span class="hljs-string">CST</span> <span class="hljs-number">2020 </span><span class="hljs-string">in</span> <span class="hljs-number">23</span> <span class="hljs-string">milliseconds</span><span class="hljs-string">The</span> <span class="hljs-string">filesystem</span> <span class="hljs-string">under</span> <span class="hljs-string">path</span> <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">is</span> <span class="hljs-string">CORRUPT</span>[<span class="hljs-string">hadoop@hadoop100</span> <span class="hljs-string">bin</span>]<span class="hljs-string">$</span> [<span class="hljs-string">hadoop@hadoop100</span> <span class="hljs-string">bin</span>]<span class="hljs-string">$</span> <span class="hljs-string">hdfs</span> <span class="hljs-string">fsck</span> <span class="hljs-string">/</span><span class="hljs-attr">20/11/29 14:30:46 WARN util.NativeCodeLoader:</span> <span class="hljs-string">Unable</span> <span class="hljs-string">to</span> <span class="hljs-string">load</span> <span class="hljs-string">native-hadoop</span> <span class="hljs-string">library</span> <span class="hljs-string">for</span> <span class="hljs-string">your</span> <span class="hljs-string">platform...</span> <span class="hljs-string">using</span> <span class="hljs-string">builtin-java</span> <span class="hljs-string">classes</span> <span class="hljs-string">where</span> <span class="hljs-string">applicable</span><span class="hljs-string">Connecting</span> <span class="hljs-string">to</span> <span class="hljs-string">namenode</span> <span class="hljs-string">via</span> <span class="hljs-string">http://hadoop100:50070/fsck?ugi=hadoop&amp;path=%2F</span><span class="hljs-string">FSCK</span> <span class="hljs-string">started</span> <span class="hljs-string">by</span> <span class="hljs-string">hadoop</span> <span class="hljs-string">(auth:SIMPLE)</span> <span class="hljs-string">from</span> <span class="hljs-string">/192.168.1.100</span> <span class="hljs-string">for</span> <span class="hljs-string">path</span> <span class="hljs-string">/</span> <span class="hljs-string">at</span> <span class="hljs-string">Sun</span> <span class="hljs-string">Nov</span> <span class="hljs-number">29</span> <span class="hljs-number">14</span><span class="hljs-string">:30:47</span> <span class="hljs-string">CST</span> <span class="hljs-number">2020</span><span class="hljs-string">.</span><span class="hljs-string">/a.log:</span> <span class="hljs-string">CORRUPT</span> <span class="hljs-string">blockpool</span> <span class="hljs-string">BP-1678315190-192.168.1.100-1606024642916</span> <span class="hljs-string">block</span> <span class="hljs-string">blk_1073741858</span><span class="hljs-string">/a.log:</span> <span class="hljs-attr">MISSING 1 blocks of total size 4 B.....................Status:</span> <span class="hljs-string">CORRUPT</span> <span class="hljs-attr">Total size:</span><span class="hljs-number">138189</span> <span class="hljs-string">B</span> <span class="hljs-attr">Total dirs:</span><span class="hljs-number">184</span> <span class="hljs-attr">Total files:</span><span class="hljs-number">21</span> <span class="hljs-attr">Total symlinks:</span><span class="hljs-number">0</span> <span class="hljs-string">Total</span> <span class="hljs-string">blocks</span> <span class="hljs-string">(validated):</span><span class="hljs-number">20</span> <span class="hljs-string">(avg.</span> <span class="hljs-string">block</span> <span class="hljs-string">size</span> <span class="hljs-number">6909 </span><span class="hljs-string">B)</span>  <span class="hljs-string">********************************</span>  <span class="hljs-string">UNDER</span> <span class="hljs-string">MIN</span> <span class="hljs-string">REPL&#x27;D</span> <span class="hljs-attr">BLOCKS:</span><span class="hljs-number">1</span> <span class="hljs-string">(5.0</span> <span class="hljs-string">%)</span>  <span class="hljs-attr">dfs.namenode.replication.min:</span><span class="hljs-number">1</span>  <span class="hljs-attr">CORRUPT FILES:</span><span class="hljs-number">1</span>  <span class="hljs-attr">MISSING BLOCKS:</span><span class="hljs-number">1</span>  <span class="hljs-attr">MISSING SIZE:</span><span class="hljs-number">4</span> <span class="hljs-string">B</span>  <span class="hljs-attr">CORRUPT BLOCKS:</span> <span class="hljs-number">1</span>  <span class="hljs-string">********************************</span> <span class="hljs-attr">Minimally replicated blocks:</span><span class="hljs-number">19</span> <span class="hljs-string">(95.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Over-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Under-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Mis-replicated blocks:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Default replication factor:</span><span class="hljs-number">1</span> <span class="hljs-attr">Average block replication:</span><span class="hljs-number">0.95</span> <span class="hljs-attr">Corrupt blocks:</span><span class="hljs-number">1</span> <span class="hljs-attr">Missing replicas:</span><span class="hljs-number">0</span> <span class="hljs-string">(0.0</span> <span class="hljs-string">%)</span> <span class="hljs-attr">Number of data-nodes:</span><span class="hljs-number">1</span> <span class="hljs-attr">Number of racks:</span><span class="hljs-number">1</span><span class="hljs-string">FSCK</span> <span class="hljs-string">ended</span> <span class="hljs-string">at</span> <span class="hljs-string">Sun</span> <span class="hljs-string">Nov</span> <span class="hljs-number">29</span> <span class="hljs-number">14</span><span class="hljs-string">:30:47</span> <span class="hljs-string">CST</span> <span class="hljs-number">2020 </span><span class="hljs-string">in</span> <span class="hljs-number">18</span> <span class="hljs-string">milliseconds</span><span class="hljs-string">The</span> <span class="hljs-string">filesystem</span> <span class="hljs-string">under</span> <span class="hljs-string">path</span> <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">is</span> <span class="hljs-string">CORRUPT</span></code></pre><h4 id="配置各个节点平衡"><a href="#配置各个节点平衡" class="headerlink" title="配置各个节点平衡"></a>配置各个节点平衡</h4><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ cat logs/hadoop-hadoop-balancer-hadoop100.log <span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">101</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs:<span class="hljs-comment">//hadoop100:9000]</span><span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">104</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = <span class="hljs-number">10.0</span>, max idle iteration = <span class="hljs-number">5</span>, #excluded nodes = <span class="hljs-number">0</span>, #included nodes = <span class="hljs-number">0</span>, #source nodes = <span class="hljs-number">0</span>, run during upgrade = <span class="hljs-literal">false</span>]<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">104</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">105</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">105</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">50</span>,<span class="hljs-number">198</span> WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library <span class="hljs-keyword">for</span> your platform... using builtin-java classes where applicable<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">033</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.movedWinWidth = <span class="hljs-number">5400000</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">5400000</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">033</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.moverThreads = <span class="hljs-number">1000</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">1000</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">034</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.dispatcherThreads = <span class="hljs-number">200</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">200</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">034</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = <span class="hljs-number">50</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">50</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">040</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.max-size-to-move = <span class="hljs-number">10737418240</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">10737418240</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">056</span> INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /<span class="hljs-keyword">default</span>-rack/<span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.100</span>:<span class="hljs-number">50010</span><span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">057</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: <span class="hljs-number">0</span> over-utilized: []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-28</span> <span class="hljs-number">22</span>:<span class="hljs-number">07</span>:<span class="hljs-number">51</span>,<span class="hljs-number">057</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: <span class="hljs-number">0</span> underutilized: []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">175</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes  = [hdfs:<span class="hljs-comment">//hadoop100:9000]</span><span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">212</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = <span class="hljs-number">10.0</span>, max idle iteration = <span class="hljs-number">5</span>, #excluded nodes = <span class="hljs-number">0</span>, #included nodes = <span class="hljs-number">0</span>, #source nodes = <span class="hljs-number">0</span>, run during upgrade = <span class="hljs-literal">false</span>]<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">212</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: included nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">212</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: excluded nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">212</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: source nodes = []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">16</span>,<span class="hljs-number">303</span> WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library <span class="hljs-keyword">for</span> your platform... using builtin-java classes where applicable<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">329</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.movedWinWidth = <span class="hljs-number">5400000</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">5400000</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">329</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.moverThreads = <span class="hljs-number">1000</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">1000</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">329</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.dispatcherThreads = <span class="hljs-number">200</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">200</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">329</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = <span class="hljs-number">50</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">50</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">335</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: dfs.balancer.max-size-to-move = <span class="hljs-number">10737418240</span> (<span class="hljs-keyword">default</span>=<span class="hljs-number">10737418240</span>)<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">351</span> INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /<span class="hljs-keyword">default</span>-rack/<span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.100</span>:<span class="hljs-number">50010</span><span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">352</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: <span class="hljs-number">0</span> over-utilized: []<span class="hljs-number">2020</span><span class="hljs-number">-11</span><span class="hljs-number">-29</span> <span class="hljs-number">14</span>:<span class="hljs-number">52</span>:<span class="hljs-number">17</span>,<span class="hljs-number">352</span> INFO org.apache.hadoop.hdfs.server.balancer.Balancer: <span class="hljs-number">0</span> underutilized: []</code></pre>threshold = 10.0<br>是指每个节点磁盘使用率-平均磁盘使用率&lt;10%<br>第一个节点使用率是 90%<br>第二个节点使用率是 60%<br>第一个节点使用率是 80%<br>平均使用率就是 230%/3=76%<br>第一个节点 90% -76% = 14%    多了4%<br>第二个节点 60% -76% = -16%   少-16%<br>第三个节点 80% -76%=  4%     满足</li><li>*【生产中可以写个脚本，每晚业务低估去执行一下】**</li><li>*【只有当机器数大于副本数，这个命令才有意义】**<h4 id="hdfs-diskbalancer-单个节点多块磁盘平衡"><a href="#hdfs-diskbalancer-单个节点多块磁盘平衡" class="headerlink" title="hdfs diskbalancer 单个节点多块磁盘平衡"></a>hdfs diskbalancer 单个节点多块磁盘平衡</h4></li><li>配置<br>假设有data01，data02，data03三个磁盘<br>在hdfs-site.xml中添加如下信息<pre><code class="hljs dts"><span class="hljs-params">&lt;property&gt;</span>      <span class="hljs-params">&lt;name&gt;</span>dfs.datanode.data.dir <span class="hljs-params">&lt;/name&gt;</span>      <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/data01/</span>dfs/dn,<span class="hljs-meta-keyword">/data02/</span>dfs/dn,<span class="hljs-meta-keyword">/data03/</span>dfs/dn<span class="hljs-params">&lt;/value&gt;</span><span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;property&gt;</span>      <span class="hljs-params">&lt;name&gt;</span>dfs.disk.balancer.enabled<span class="hljs-params">&lt;/name&gt;</span>      <span class="hljs-params">&lt;value&gt;</span>true<span class="hljs-params">&lt;/value&gt;</span><span class="hljs-params">&lt;/property&gt;</span></code></pre></li><li>命令查看<pre><code class="hljs smali">[hadoop@hadoop100 hadoop]$ hdfs diskbalancerusage: hdfs diskbalancer [command] [options]DiskBalancer distributes data evenly between different disks on adatanode. DiskBalancer operates by generating a plan, that tells datanodehow to<span class="hljs-built_in"> move </span>data between disks. Users can<span class="hljs-built_in"> execute </span>a plan by submitting itto the datanode.To get specific help on a particular command please runhdfs diskbalancer -help &lt;command&gt;.  --help &lt;arg&gt;   valid commands are plan |<span class="hljs-built_in"> execute </span>| query | cancel |                 report</code></pre></li><li>版本支持<br>Apache hadoop2.x 不支持该命令<br>Apache hadoop3.x 支持该命令<br>CDH hadoop2.x 支持该命令</li><li>执行命令<br>hdfs diskbalancer -plan 主机名</li><li>生产中写脚本每晚在业务低估去执行该命令</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>hdfs</tag>
      
      <tag>hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop额外的配置细节及注意事项</title>
    <link href="/2020/11/17/Hadoop%E9%A2%9D%E5%A4%96%E7%9A%84%E9%85%8D%E7%BD%AE%E7%BB%86%E8%8A%82%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
    <url>/2020/11/17/Hadoop%E9%A2%9D%E5%A4%96%E7%9A%84%E9%85%8D%E7%BD%AE%E7%BB%86%E8%8A%82%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="重新调整yarn端口号"><a href="#重新调整yarn端口号" class="headerlink" title="重新调整yarn端口号"></a>重新调整yarn端口号</h4><p>yarn 容易被挖矿，需要调整默认的8080端口，在yarn-site.xml中新增：</p><pre><code class="hljs dts">[hadoop@hadoop100 hadoop]$ vim etc<span class="hljs-meta-keyword">/hadoop/</span>yarn-site.xml    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>yarn.resourcemanager.webapp.address<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>$&#123;yarn.resourcemanager.hostname&#125;:<span class="hljs-number">7776</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span></code></pre><p>修改完端口号后可以在网址<a href="http://hadoop100:7776/cluster%E4%B8%AD%E8%AE%BF%E9%97%AEresourcemanager">http://hadoop100:7776/cluster中访问resourcemanager</a></p><h4 id="jps命令"><a href="#jps命令" class="headerlink" title="jps命令"></a>jps命令</h4><h5 id="位置"><a href="#位置" class="headerlink" title="位置"></a>位置</h5><pre><code class="hljs awk">[hadoop@hadoop100 hadoop]$ which jps<span class="hljs-regexp">/usr/</span>java<span class="hljs-regexp">/jdk1.8.0_181/</span>bin/jps</code></pre><h5 id="jps命令的使用及对应的表示文件位置"><a href="#jps命令的使用及对应的表示文件位置" class="headerlink" title="jps命令的使用及对应的表示文件位置"></a>jps命令的使用及对应的表示文件位置</h5><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ jps<span class="hljs-number">6048</span> ResourceManager<span class="hljs-number">5522</span> NameNode<span class="hljs-number">5828</span> SecondaryNameNode<span class="hljs-number">6229</span> NodeManager<span class="hljs-number">5639</span> DataNode<span class="hljs-number">6636</span> Jps<span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ cd /tmp/hsperfdata_hadoop/<span class="hljs-string">[hadoop@hadoop100 hsperfdata_hadoop]</span>$ ll总用量 <span class="hljs-number">160</span>-rw-------. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">32768</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">12</span> <span class="hljs-number">5522</span>-rw-------. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">32768</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">12</span> <span class="hljs-number">5639</span>-rw-------. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">32768</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">12</span> <span class="hljs-number">5828</span>-rw-------. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">32768</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">11</span> <span class="hljs-number">6048</span>-rw-------. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">32768</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">11</span> <span class="hljs-number">6229</span></code></pre><h5 id="哪个用户可以查看"><a href="#哪个用户可以查看" class="headerlink" title="哪个用户可以查看"></a>哪个用户可以查看</h5><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># jps<span class="hljs-number">6048</span> ResourceManager<span class="hljs-number">5522</span> NameNode<span class="hljs-number">5828</span> SecondaryNameNode<span class="hljs-number">6229</span> NodeManager<span class="hljs-number">5639</span> DataNode<span class="hljs-number">6767</span> Jps<span class="hljs-string">[root@hadoop100 ~]</span># su - qi <span class="hljs-string">[qi@hadoop100 ~]</span>$ jps<span class="hljs-number">6837</span> Jps</code></pre><p>可以看出，hadoop和root用户可以查看，但是其他的普通用户查看不了</p><h5 id="出现process-information-unavailable描述"><a href="#出现process-information-unavailable描述" class="headerlink" title="出现process information unavailable描述"></a>出现process information unavailable描述</h5><p>模拟出现</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># jps<span class="hljs-number">6048</span> ResourceManager<span class="hljs-number">6976</span> Jps<span class="hljs-number">5522</span> NameNode<span class="hljs-number">5828</span> SecondaryNameNode<span class="hljs-number">6229</span> NodeManager<span class="hljs-number">5639</span> DataNode<span class="hljs-string">[root@hadoop100 ~]</span># kill <span class="hljs-number">-9</span> <span class="hljs-number">5828</span><span class="hljs-string">[root@hadoop100 ~]</span># jps<span class="hljs-number">6048</span> ResourceManager<span class="hljs-number">5522</span> NameNode<span class="hljs-number">5828</span> -- process information unavailable<span class="hljs-number">6229</span> NodeManager<span class="hljs-number">5639</span> DataNode<span class="hljs-number">7001</span> Jps</code></pre><p>当看见 process information unavailable，不能代表进程是存在或者不存在，要当心，尤其使用jps命令来做脚本状态检测的一般使用经典的 ps -ef | grep xxx命令去查看进程是否存在，这才是真正的状态检测。<br>但是: 比如spark thriftserver +hive 会启动一个driver 进程 110，默认端口号 10001。由于该程序的内存泄露或者某种bug，导致进程ps是存在的，10001端口号下线了，就不能够对外提供服务。</p><p>总结: 未来做任何程序的状态检测，必须通过端口号来。</p><p>CDH集群中的root用户，jps命令查看会有很多的 process information unavailable，经过ps -ef| grep xxx 命令查看是正确的，那么想要看到正常的表述，需要切换对应的用户，<br>比如su - hdfs(有可能你切换不过去，需要/etc/passwd文件的修正(用户后面修改为/bin/bash))，再执行jps命令。</p><h4 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h4><h5 id="pid文件的位置"><a href="#pid文件的位置" class="headerlink" title="pid文件的位置"></a>pid文件的位置</h5><p>默认是配置在/tmp目录中</p><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ ll总用量 <span class="hljs-number">20</span>drwx------. <span class="hljs-number">2</span> root    root    <span class="hljs-number">25</span> <span class="hljs-number">11</span>月 <span class="hljs-number">16</span> <span class="hljs-number">13</span>:<span class="hljs-number">10</span> firefox_root-rw-rw-r--. <span class="hljs-number">1</span> hadoop  hadoop   <span class="hljs-number">5</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">25</span> hadoop-hadoop-datanode.pid-rw-rw-r--. <span class="hljs-number">1</span> hadoop  hadoop   <span class="hljs-number">5</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">25</span> hadoop-hadoop-namenode.pid-rw-rw-r--. <span class="hljs-number">1</span> hadoop  hadoop   <span class="hljs-number">5</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">25</span> hadoop-hadoop-secondarynamenode.pid<span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ cat hadoop-hadoop-datanode.pid <span class="hljs-number">8146</span><span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ jps<span class="hljs-number">8705</span> NodeManager<span class="hljs-number">8146</span> DataNode</code></pre><p>现在修改datanode的pid文件，再执行jps命令，再将hadoop集群停止，再执行jps命令查看结果</p><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ mv hadoop-hadoop-datanode.pid hadoop-hadoop-datanode.pid.bak<span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ jps<span class="hljs-number">8705</span> NodeManager<span class="hljs-number">8146</span> DataNode<span class="hljs-number">8345</span> SecondaryNameNode<span class="hljs-number">8521</span> ResourceManager<span class="hljs-number">7997</span> NameNode<span class="hljs-number">9199</span> Jps<span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ stop-all.shhadoop100: no datanode to stop<span class="hljs-string">[hadoop@hadoop100 tmp]</span>$ jps<span class="hljs-number">8146</span> DataNode<span class="hljs-number">9786</span> Jps</code></pre><p>当修改完datanode的pid文件后，停止集群并执行jps命令，发现datanode进程并没有被杀死，执行ps -ef|grep命令查看一下DataNode进程</p><pre><code class="hljs routeros">[hadoop@hadoop100 tmp]$ ps -ef|grep 8146hadoop     8146      1  1 00:25 ?        00:00:07 /usr/java/jdk1.8.0_181/bin/java -Dproc_datanode -Xmx1000m -Djava.net.<span class="hljs-attribute">preferIPv4Stack</span>=<span class="hljs-literal">true</span> -org.apache.hadoop.hdfs.server.datanode.DataNode<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.<span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span><span class="hljs-built_in">..</span>.hadoop     9834   2601  0 00:33 pts/0    00:00:00 grep <span class="hljs-attribute">--color</span>=auto 8146</code></pre><p>结果显示DataNode进程确实没有被杀死</p><h5 id="pid文件被删除后会影响服务重启"><a href="#pid文件被删除后会影响服务重启" class="headerlink" title="pid文件被删除后会影响服务重启"></a>pid文件被删除后会影响服务重启</h5><p>因为pid默认是配置在/tmp目录下，30天会清理一次，因此，pid文件不应该配置在/tmp目录中，需要在hadoop-env.sh中修改配置</p><pre><code class="hljs routeros">[hadoop@hadoop100 hadoop]$ vim etc/hadoop/hadoop-env.sh<span class="hljs-comment"># The directory where pid files are stored. /tmp by default.</span><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> this should be set to a directory that can only be written to by </span><span class="hljs-comment">#       the user that will run the hadoop daemons.  Otherwise there is the</span><span class="hljs-comment">#       potential for a symlink attack.</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">HADOOP_PID_DIR</span>=/home/hadoop/tmp</code></pre><p>修改完后，重新启动集群，查看修改后的目录中是否存在pid文件</p><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ start-dfs.sh<span class="hljs-string">[hadoop@hadoop100 ~]</span>$ ll /home/hadoop/tmp/-rw-rw-r--. <span class="hljs-number">1</span> hadoop hadoop  <span class="hljs-number">6</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">52</span> hadoop-hadoop-datanode.pid-rw-rw-r--. <span class="hljs-number">1</span> hadoop hadoop  <span class="hljs-number">6</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">52</span> hadoop-hadoop-namenode.pid-rw-rw-r--. <span class="hljs-number">1</span> hadoop hadoop  <span class="hljs-number">6</span> <span class="hljs-number">11</span>月 <span class="hljs-number">24</span> <span class="hljs-number">00</span>:<span class="hljs-number">52</span> hadoop-hadoop-secondarynamenode.pid</code></pre><h4 id="block块"><a href="#block块" class="headerlink" title="block块"></a>block块</h4><p>HDFS中的文件在物理上是分块存储（Block）,块大小可以通过配置参数（dfs.blocksize）来规定，默认在Hadoop2.x版本中是128M，老板本是64M。<br>比如一个文件大小是260M，需要3个块来存储，第一二个块分别存储128M，第三个块存储4M。</p><p>为什么默认的块大小设置为128M？<br>1、    如果寻址时间约为10ms，即找到目标block块的时间为10ms<br>2、    寻址时间为传输时间的1%时，为最佳状态，因此传输时间为10ms/0.01=1s<br>3、    而目前磁盘的传输速率普遍为 100MB/S<br>4、    Block块大小=1s * 100MB/s=100MB<br>如果块设置太小，会增加寻址时间，程序一直在找块的开始位置<br>如果块设置太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间，导致程序处理这块数据时，会非常慢</p><p>总结：HDFS块的大小设置主要取决于磁盘的传输速率</p><h4 id="小文件"><a href="#小文件" class="headerlink" title="小文件"></a>小文件</h4><p>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。<br>解决办法：<br>1、    将小文件合并成大文件再上传到HDFS上<br>2、    使用har归档，Hadoop存档文件对内是一个个独立文件，对NN而言是一个整体，减少了NN的内存</p><h4 id="配置回收站"><a href="#配置回收站" class="headerlink" title="配置回收站"></a>配置回收站</h4><p>在配置文件 core-site.xml中添加以下内容</p><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 配置回收站，保存7天. --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.trash.interval<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>10080<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre><p>现在删除一个文件会先放到回收站中</p><pre><code class="hljs awk">[hadoop@hadoop100 hadoop]$ hadoop fs -rm -R /tmp<span class="hljs-number">20</span><span class="hljs-regexp">/11/</span><span class="hljs-number">29</span> <span class="hljs-number">14</span>:<span class="hljs-number">48</span>:<span class="hljs-number">24</span> INFO fs.TrashPolicyDefault:Moved: <span class="hljs-string">&#x27;hdfs://hadoop100:9000/tmp&#x27;</span> to trash at: hdfs:<span class="hljs-regexp">//</span>hadoop100:<span class="hljs-number">9000</span><span class="hljs-regexp">/user/</span>hadoop<span class="hljs-regexp">/.Trash/</span>Current/tmp</code></pre><h4 id="为什么机器上部署DN-NM进程，一般都是两者一起"><a href="#为什么机器上部署DN-NM进程，一般都是两者一起" class="headerlink" title="为什么机器上部署DN NM进程，一般都是两者一起?"></a>为什么机器上部署DN NM进程，一般都是两者一起?</h4><p>一般 NodeManager 节点执行，需要从 DataNode 节点拉取数据，假如计算时当前节点正好有数据块，就没有必要从其他节点拉取数据，节省了网络传输，这叫数据本地化</p>]]></content>
    
    
    
    <tags>
      
      <tag>hdfs</tag>
      
      <tag>hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL between and 查询范围</title>
    <link href="/2020/11/17/MySQL%20between%20and%20%E6%9F%A5%E8%AF%A2%E8%8C%83%E5%9B%B4/"/>
    <url>/2020/11/17/MySQL%20between%20and%20%E6%9F%A5%E8%AF%A2%E8%8C%83%E5%9B%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="数字范围"><a href="#数字范围" class="headerlink" title="数字范围"></a>数字范围</h4><ul><li><p>between a and b，范围是[a,b]</p><pre><code class="hljs n1ql"><span class="hljs-keyword">select</span> age <span class="hljs-keyword">from</span> rz <span class="hljs-keyword">where</span> age <span class="hljs-keyword">between</span> <span class="hljs-number">18</span> <span class="hljs-keyword">and</span> <span class="hljs-number">20</span>;</code></pre><p><img src="./images/1605941715988.png" alt="enter description here"></p><h4 id="时间范围"><a href="#时间范围" class="headerlink" title="时间范围"></a>时间范围</h4></li><li><p>数据类型是datetime类型<br>查询参数带时分秒，between and 范围相当于[a,b]</p><pre><code class="hljs apache"><span class="hljs-attribute">select</span> * from time_<span class="hljs-number">1</span> where time_ between &#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">22</span> <span class="hljs-number">09</span>:<span class="hljs-number">39</span>:<span class="hljs-number">33</span>&#x27; and &#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">25</span> <span class="hljs-number">15</span>:<span class="hljs-number">17</span>:<span class="hljs-number">51</span>&#x27;;</code></pre><p><img src="./images/1605942791033.png" alt="enter description here"></p><p>查询参数不带时分秒，between and 范围相当于[a,b)</p><pre><code class="hljs apache"><span class="hljs-attribute">select</span> * from time_<span class="hljs-number">1</span> where time_ between &#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">22</span>&#x27; and &#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">25</span>&#x27;;</code></pre><p><img src="./images/1605942949326.png" alt="enter description here"></p></li><li><p>数据类型是date类型</p><pre><code class="hljs apache"><span class="hljs-attribute">select</span> * from time_<span class="hljs-number">2</span> where time_ between &#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">26</span>&#x27; and (&#x27;<span class="hljs-number">2019</span>-<span class="hljs-number">03</span>-<span class="hljs-number">28</span>&#x27;);</code></pre><p><img src="./images/1605943126305.png" alt="enter description here"></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL行转列，列转行</title>
    <link href="/2020/11/17/MySQL%E8%A1%8C%E5%88%97%E8%BD%AC%E8%A1%8C/"/>
    <url>/2020/11/17/MySQL%E8%A1%8C%E5%88%97%E8%BD%AC%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<hr><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>建表、插入并查看数据</p><pre><code class="hljs routeros">CREATE TABLE `TEST_TB_GRADE` ( `ID` int(10) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span> AUTO_INCREMENT, `USER_NAME` varchar(20)<span class="hljs-built_in"> DEFAULT </span><span class="hljs-literal">NULL</span>, `COURSE` varchar(20)<span class="hljs-built_in"> DEFAULT </span><span class="hljs-literal">NULL</span>, `SCORE` float<span class="hljs-built_in"> DEFAULT </span><span class="hljs-string">&#x27;0&#x27;</span>, PRIMARY KEY (`ID`)) <span class="hljs-attribute">ENGINE</span>=InnoDB <span class="hljs-attribute">AUTO_INCREMENT</span>=1<span class="hljs-built_in"> DEFAULT </span><span class="hljs-attribute">CHARSET</span>=utf8;insert into TEST_TB_GRADE(USER_NAME, COURSE, SCORE) values(<span class="hljs-string">&quot;张三&quot;</span>, <span class="hljs-string">&quot;数学&quot;</span>, 34),(<span class="hljs-string">&quot;张三&quot;</span>, <span class="hljs-string">&quot;语文&quot;</span>, 58),(<span class="hljs-string">&quot;张三&quot;</span>, <span class="hljs-string">&quot;英语&quot;</span>, 58),(<span class="hljs-string">&quot;李四&quot;</span>, <span class="hljs-string">&quot;数学&quot;</span>, 45),(<span class="hljs-string">&quot;李四&quot;</span>, <span class="hljs-string">&quot;语文&quot;</span>, 87),(<span class="hljs-string">&quot;李四&quot;</span>, <span class="hljs-string">&quot;英语&quot;</span>, 45),(<span class="hljs-string">&quot;王五&quot;</span>, <span class="hljs-string">&quot;数学&quot;</span>, 76),(<span class="hljs-string">&quot;王五&quot;</span>, <span class="hljs-string">&quot;语文&quot;</span>, 34),(<span class="hljs-string">&quot;王五&quot;</span>, <span class="hljs-string">&quot;英语&quot;</span>, 89);select * <span class="hljs-keyword">from</span> TEST_TB_GRADE;</code></pre><p>.<img src="./images/1606031833815.png" alt="enter description here"><br>行转列：</p><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> user_name ,  <span class="hljs-keyword">MAX</span>(<span class="hljs-keyword">CASE</span> course <span class="hljs-keyword">WHEN</span> <span class="hljs-string">&#x27;数学&#x27;</span> <span class="hljs-keyword">THEN</span> score <span class="hljs-keyword">ELSE</span> <span class="hljs-number">0</span> <span class="hljs-keyword">END</span> ) 数学,  <span class="hljs-keyword">MAX</span>(<span class="hljs-keyword">CASE</span> course <span class="hljs-keyword">WHEN</span> <span class="hljs-string">&#x27;语文&#x27;</span> <span class="hljs-keyword">THEN</span> score <span class="hljs-keyword">ELSE</span> <span class="hljs-number">0</span> <span class="hljs-keyword">END</span> ) 语文,  <span class="hljs-keyword">MAX</span>(<span class="hljs-keyword">CASE</span> course <span class="hljs-keyword">WHEN</span> <span class="hljs-string">&#x27;英语&#x27;</span> <span class="hljs-keyword">THEN</span> score <span class="hljs-keyword">ELSE</span> <span class="hljs-number">0</span> <span class="hljs-keyword">END</span> ) 英语<span class="hljs-keyword">FROM</span> test_tb_grade<span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> USER_NAME;</code></pre><p>.<img src="./images/1606031889994.png" alt="enter description here"></p><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>建表、插入并查看数据</p><pre><code class="hljs routeros">CREATE TABLE `TEST_TB_GRADE2` ( `ID` int(10) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span> AUTO_INCREMENT, `USER_NAME` varchar(20)<span class="hljs-built_in"> DEFAULT </span><span class="hljs-literal">NULL</span>, `CN_SCORE` float<span class="hljs-built_in"> DEFAULT </span><span class="hljs-literal">NULL</span>, `MATH_SCORE` float<span class="hljs-built_in"> DEFAULT </span><span class="hljs-literal">NULL</span>, `EN_SCORE` float<span class="hljs-built_in"> DEFAULT </span><span class="hljs-string">&#x27;0&#x27;</span>, PRIMARY KEY (`ID`)) <span class="hljs-attribute">ENGINE</span>=InnoDB <span class="hljs-attribute">AUTO_INCREMENT</span>=1<span class="hljs-built_in"> DEFAULT </span><span class="hljs-attribute">CHARSET</span>=utf8;insert into TEST_TB_GRADE2(USER_NAME, CN_SCORE, MATH_SCORE, EN_SCORE) values(<span class="hljs-string">&quot;张三&quot;</span>, 34, 58, 58),(<span class="hljs-string">&quot;李四&quot;</span>, 45, 87, 45),(<span class="hljs-string">&quot;王五&quot;</span>, 76, 34, 89);select * <span class="hljs-keyword">from</span> TEST_TB_GRADE2;</code></pre><p><img src="./images/1606032015361.png" alt="enter description here"><br>列转行</p><pre><code class="hljs crystal"><span class="hljs-keyword">select</span> user_name, <span class="hljs-string">&#x27;语文&#x27;</span> COURSE , CN_SCORE <span class="hljs-keyword">as</span> SCORE from test_tb_grade2<span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">select</span> <span class="hljs-title">user_name</span>, &#x27;数学&#x27; <span class="hljs-title">COURSE</span>, <span class="hljs-title">MATH_SCORE</span> <span class="hljs-title">as</span> <span class="hljs-title">SCORE</span> <span class="hljs-title">from</span> <span class="hljs-title">test_tb_grade2</span></span><span class="hljs-class"><span class="hljs-keyword">union</span> <span class="hljs-title">select</span> <span class="hljs-title">user_name</span>, &#x27;英语&#x27; <span class="hljs-title">COURSE</span>, <span class="hljs-title">EN_SCORE</span> <span class="hljs-title">as</span> <span class="hljs-title">SCORE</span> <span class="hljs-title">from</span> <span class="hljs-title">test_tb_grade2</span></span>order by user_name,COURSE;</code></pre><p>.<img src="./images/1606032072162.png" alt="enter description here"></p>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce</title>
    <link href="/2020/11/17/MapReduce/"/>
    <url>/2020/11/17/MapReduce/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul><li>mapreduce是一个分布式计算框架</li><li>map 映射：指的是一组数据按照规则映射为一组<br>eg：string = a b a<br>(a,1)、(b,1)、(a,1)</li><li>reduce 归约：汇总<br>(a,2)、(b,1)</li><li>shuffle 洗牌：数据根据key进行网络传输规整到一起，按规则计算<h4 id="mapreduce-on-yarn-提交流程"><a href="#mapreduce-on-yarn-提交流程" class="headerlink" title="mapreduce on yarn 提交流程"></a>mapreduce on yarn 提交流程</h4><h5 id="container-容器"><a href="#container-容器" class="headerlink" title="container 容器"></a>container 容器</h5></li><li>是一定的内存和cpu的资源组合</li><li>在内存够的情况下，适当增加cpu vcore来提升计算的并行度，提高效率</li><li>预留10% - 15%的空间</li><li>oom-kill机制：会监控那些占用内存过大，尤其是瞬间占用内存很快的进程，然后防止内存耗尽而自动把该进程杀掉</li><li>系统需要的内存+预留的内存，为未来新增服务提供所需内存；为了防止出现oom-kill机制<h5 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h5><img src="./images/1606705850062.png" alt="![enter description here](./images/1606705540653.png)"></li><li>*启动applicationmaster主程序，领取资源**</li></ul><ol><li>Client 向RM提交应用程序，其中包含applicationmaster主程序和启动命令</li><li>applications manager 会为【应用程序分配第一个container容器】，来运行applicationmaster主程序</li><li>applicationmaster主程序就会向applications manager 注册，就可以做yarn的web界面上看到job的运行状态</li><li>applicationmaster主程序采取轮询的方式通过【rpc】协议向resourcescheduler，申请和领取资源(哪台机器 领取多少内存 多少cpu VCORE)</li></ol><p><strong>启动applicationmaster主程序，领取资源</strong></p><ol start="5"><li>一旦applicationmaster主程序拿到资源的列表，就和对应的nm进程进行通信，要求启动container来运行task任务。</li><li>nm就为task任务设置好运行的环境（container容器）将任务启动命令写在脚本里，并且通过脚本启动任务task。</li><li>各个container的task 任务(map task、reduce task任务)，通过【rpc】协议向applicationmaster主程序进行汇报进度和状态，以此让applicationmaster主程序随时掌握task的运行状态。当task任务运行失败，也会重启container任务。</li><li>当所有的task任务全部完成，applicationmaster主程序会向applications manager 申请注销和关闭作业，这时在web界面查看任务是  是否完成 ，是成功还是失败。</li></ol><p><strong>注意点</strong></p><ul><li>主程序在 NodeManager 所在节点运行</li><li>主程序要向 ResourceSheduler 申请 container 容器</li><li>一个作业第一个 container 容器运行主程序</li></ul><h4 id="小文件危害"><a href="#小文件危害" class="headerlink" title="小文件危害"></a>小文件危害</h4><p>假设有10个1M的小文件<br>根据默认的split切分规则，会生成10个map task也就对应有10个 container容器，那么必然会造成资源的浪费；<br>如果将10个小文件合并成一个10M的文件，切分后就只有一个map task对应一个container容器，就可以节省很多资源。<br><strong>总结：小文件危害</strong></p><ul><li>在存储上，小文件过多会对NameNode产生很大的压力</li><li>在计算书，小文件过多会造成资源的浪费</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>hadoop</tag>
      
      <tag>mapreduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL分组求TopN</title>
    <link href="/2020/11/17/MySQL%E5%88%86%E7%BB%84%E6%B1%82TopN/"/>
    <url>/2020/11/17/MySQL%E5%88%86%E7%BB%84%E6%B1%82TopN/</url>
    
    <content type="html"><![CDATA[<hr><p>建表，插入数据，查看数据</p><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> test1(<span class="hljs-keyword">sid</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">comment</span> <span class="hljs-string">&quot;学生ID&quot;</span>,cid <span class="hljs-built_in">varchar</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">comment</span> <span class="hljs-string">&quot;课程ID&quot;</span>,score <span class="hljs-built_in">decimal</span>(<span class="hljs-number">18</span>,<span class="hljs-number">1</span>) <span class="hljs-keyword">comment</span> <span class="hljs-string">&quot;课程成绩&quot;</span>););<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test1 <span class="hljs-keyword">values</span>(<span class="hljs-string">&#x27;01&#x27;</span> , <span class="hljs-string">&#x27;01&#x27;</span> , <span class="hljs-number">80</span>),(<span class="hljs-string">&#x27;02&#x27;</span> , <span class="hljs-string">&#x27;01&#x27;</span> , <span class="hljs-number">81</span>),(<span class="hljs-string">&#x27;03&#x27;</span> , <span class="hljs-string">&#x27;01&#x27;</span> , <span class="hljs-number">82</span>),(<span class="hljs-string">&#x27;01&#x27;</span> , <span class="hljs-string">&#x27;02&#x27;</span> , <span class="hljs-number">99</span>),(<span class="hljs-string">&#x27;02&#x27;</span> , <span class="hljs-string">&#x27;02&#x27;</span> , <span class="hljs-number">92</span>),(<span class="hljs-string">&#x27;03&#x27;</span> , <span class="hljs-string">&#x27;02&#x27;</span> , <span class="hljs-number">90</span>);<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test1;</code></pre><p>.<img src="./images/1606028971441.png" alt="enter description here"></p><p><strong>求每门课程中成绩最好的人 Top1</strong></p><pre><code class="hljs routeros">select test1.*<span class="hljs-keyword">from</span> test1 inner join(select cid,max(score) max_score <span class="hljs-keyword">from</span> test1group by cid) t1on t1.cid = test1.cid <span class="hljs-keyword">and</span> t1.max_score = test1.score</code></pre><p>.<img src="./images/1606029043413.png" alt="enter description here"><br>求Top1的原理很简单，先从原表中找到各个课程的最大值，然后在将查询结果与原表做一个内连接就可以定位到每个课程的最大值的信息了<br><strong>求每门课程中成绩最好的前两名 Top2</strong></p><pre><code class="hljs mipsasm">select sid,cid,<span class="hljs-keyword">score</span><span class="hljs-keyword">from </span>(select <span class="hljs-built_in">t1</span>.* from test1 <span class="hljs-built_in">t1</span>left <span class="hljs-keyword">join </span>test1 <span class="hljs-built_in">t2</span>on <span class="hljs-built_in">t1</span>.cid = <span class="hljs-built_in">t2</span>.cid <span class="hljs-keyword">and </span><span class="hljs-built_in">t1</span>.<span class="hljs-keyword">score </span>&lt; <span class="hljs-built_in">t2</span>.<span class="hljs-keyword">score</span><span class="hljs-keyword">order </span><span class="hljs-keyword">by </span><span class="hljs-built_in">t1</span>.<span class="hljs-keyword">score </span>desc) newgroup <span class="hljs-keyword">by </span>cid,sid,<span class="hljs-keyword">score</span><span class="hljs-keyword">having </span><span class="hljs-built_in">count</span>(cid) &lt; <span class="hljs-number">2</span><span class="hljs-keyword">order </span><span class="hljs-keyword">by </span>cid;</code></pre><p>.<img src="./images/1606030627082.png" alt="enter description here"><br>原理如下：<br>内层查找的结果如下：</p><pre><code class="hljs mipsasm">select * from test1 <span class="hljs-built_in">t1</span>left <span class="hljs-keyword">join </span>test1 <span class="hljs-built_in">t2</span>on <span class="hljs-built_in">t1</span>.cid = <span class="hljs-built_in">t2</span>.cid <span class="hljs-keyword">and </span><span class="hljs-built_in">t1</span>.<span class="hljs-keyword">score </span>&lt; <span class="hljs-built_in">t2</span>.<span class="hljs-keyword">score</span><span class="hljs-keyword">order </span><span class="hljs-keyword">by </span><span class="hljs-built_in">t1</span>.<span class="hljs-keyword">score </span>desc</code></pre><p>.<img src="./images/1606030944304.png" alt="enter description here"><br>该结果表示的对每门课程的每个学生查找该门课程中成绩是否有比他低的学生，如果有N个，则该同学的信息会出现N次，如果匹配不到则出现1次（他本身）<br>例如：<br>sid为03，cid为02的学生，他的成绩是90，在cid为02的学生中，比90分高的学生有两个，所以结果会出现两次。<br><img src="./images/1606031294785.png" alt="enter description here"><br>最后在对内层查询的结果进行子查询，根据sid，cid，score分组，根据出现的次数就可以求出每门课程成绩最高的前2名学生（having count(cid) &lt; 2）<br>还是拿sid为03，cid为02的学生来举例，他的成绩为90，出现了2次，表示在cid为02的课程中有2名学生比他高，也就是说他是cid为02课程中排名第3的学生，因此分组后having count(cid) &lt; 2会将他排除在外。</p><p>到此，分组排序讲解完毕，如果还不理解的话，可以自己实现一下这个过程，在分析分析！</p>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Yarn调优</title>
    <link href="/2020/11/17/Yarn%E8%B0%83%E4%BC%98/"/>
    <url>/2020/11/17/Yarn%E8%B0%83%E4%BC%98/</url>
    
    <content type="html"><![CDATA[<hr><h4 id="container容器"><a href="#container容器" class="headerlink" title="container容器"></a>container容器</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><ol><li>是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</li></ol><h5 id="关于yarn调优，其实就是调整-container（生产上如何调优）"><a href="#关于yarn调优，其实就是调整-container（生产上如何调优）" class="headerlink" title="关于yarn调优，其实就是调整 container（生产上如何调优）"></a>关于yarn调优，其实就是调整 container（生产上如何调优）</h5><ol><li>假设某台机器运行内存为128G，物理core个数为 16，首先系统要预留 20% 内存给当前进程服务，防止出现 oom-kill 机制，给未来部署的软件预留空间 128 * 0.2 = 25.6，还剩下大概102G的内存</li><li>DataNode 和 NodeManager节点各分配 2G、4G的内存，现在一共剩下 102-2-4=96G内存，这96G是负责计算的container容器真正的总内存</li><li>container内存分配的相关参数（如果是96G内存）<ul><li>yarn.nodemanager.resource.memory-mb 96G（默认 8G）</li><li>yarn.scheduler.minimum-allocation-mb 1G（默认 1G）极限情况96个容器内存各1G</li><li>yarn.scheduler.maximum-allocation-mb 96G（默认 8G）极限情况1个容器内存96G</li><li>container容器会不断的自动增加内存1G</li></ul></li><li>container 虚拟core<br>这个概念是yarn自己引入的，设计初衷是考虑不同的服务器的cpu性能不一样，每个cpu计算能力不一样。比如某个物理cpu是另外一个物理cpu的2倍，这时通过设置第一个物理cpu的虚拟core来弥补差异。后来演变为大家都是在使用虚拟core，默认值是2<ul><li>yarn.nodemanager.resource.cpu-vcores   32（默认8个）</li><li>yarn.scheduler.minimum-allocation-vcores 1（默认1个）极限情况下是32个</li><li>yarn.scheduler.maximum-allocation-vcores 32 （默认4个）极限情况下是1个</li><li>yarn.nodemanager.pmem-check-enabled   true</li><li>yarn.nodemanager.vmem-check-enabled   true （默认是false）</li><li>yarn.nodemanager.vmem-pmem-ratio    2.1 不要改</li></ul></li><li>生产上如何设置container容器各个参数<br>CDH官方经过大量验证，经验值，container容器最大分配vcore不要超过5，故一般生产上设置 yarn.scheduler.maximum-allocation-vcores 4 <ul><li>yarn.nodemanager.resource.cpu-vcores            32</li><li>yarn.scheduler.minimum-allocation-vcores    1</li><li>yarn.scheduler.maximum-allocation-vcores   4 极限情况下，是只有8个container容器</li><li>yarn.nodemanager.resource.memory-mb     96G</li><li>yarn.scheduler.minimum-allocation-mb    1G</li><li>yarn.scheduler.maximum-allocation-mb  12G 极限情况下，是只有8个container容器</li><li>正常来说生产上yarn.scheduler.maximum-allocation-mb 参数设置8左右</li><li>也就是说：1个container容器12G，4个vcore，1个vcore使用3G</li></ul></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>hadoop</tag>
      
      <tag>yarn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop伪分布式部署</title>
    <link href="/2020/11/17/hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/11/17/hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="创建相应用户及文件"><a href="#创建相应用户及文件" class="headerlink" title="创建相应用户及文件"></a>创建相应用户及文件</h4><pre><code class="hljs crystal">[root@hadoop100 ~]<span class="hljs-comment"># useradd hadoop</span>[root@hadoop100 ~]<span class="hljs-comment"># su - hadoop</span>[hadoop@hadoop100 ~]$ mkdir sourcecode software app log <span class="hljs-class"><span class="hljs-keyword">lib</span> <span class="hljs-title">data</span> <span class="hljs-title">tmp</span> <span class="hljs-title">shell</span></span></code></pre><h4 id="解压jar包"><a href="#解压jar包" class="headerlink" title="解压jar包"></a>解压jar包</h4><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 ~]</span>$ cd software/<span class="hljs-string">[hadoop@hadoop100 software]</span>$ ll总用量 <span class="hljs-number">424176</span>-rw-r--r--. <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">434354462</span> <span class="hljs-number">11</span>月 <span class="hljs-number">22</span> <span class="hljs-number">13</span>:<span class="hljs-number">03</span> hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span>.tar.gz<span class="hljs-string">[hadoop@hadoop100 software]</span>$ tar -zxvf hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span>.tar.gz -C ../app/</code></pre><h4 id="部署jdk-前文中已有，若没有部署，请查看前文"><a href="#部署jdk-前文中已有，若没有部署，请查看前文" class="headerlink" title="部署jdk(前文中已有，若没有部署，请查看前文)"></a>部署jdk(前文中已有，若没有部署，请查看前文)</h4><p>查看jdk</p><pre><code class="hljs awk">[hadoop@hadoop100 software]$ which java<span class="hljs-regexp">/usr/</span>java<span class="hljs-regexp">/jdk1.8.0_181/</span>bin/java</code></pre><h4 id="添加软链接"><a href="#添加软链接" class="headerlink" title="添加软链接"></a>添加软链接</h4><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 software]</span>$ cd ../app/<span class="hljs-string">[hadoop@hadoop100 app]</span>$ ll总用量 <span class="hljs-number">0</span>drwxr-xr-x. <span class="hljs-number">14</span> hadoop hadoop <span class="hljs-number">241</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span><span class="hljs-string">[hadoop@hadoop100 app]</span>$ ln -s hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span>/ hadoop<span class="hljs-string">[hadoop@hadoop100 app]</span>$ ll总用量 <span class="hljs-number">0</span>lrwxrwxrwx.  <span class="hljs-number">1</span> hadoop hadoop  <span class="hljs-number">23</span> <span class="hljs-number">11</span>月 <span class="hljs-number">22</span> <span class="hljs-number">13</span>:<span class="hljs-number">18</span> hadoop -&gt; hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span>/drwxr-xr-x. <span class="hljs-number">14</span> hadoop hadoop <span class="hljs-number">241</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> hadoop<span class="hljs-number">-2.6</span><span class="hljs-number">.0</span>-cdh5<span class="hljs-number">.16</span><span class="hljs-number">.2</span></code></pre><p>配置软链接的好处：</p><ul><li>如果在脚本应用是hadoop，那么当hadoop的版本更换后我们只需要更改一下软链接就可以，脚本中的不需要更改，脚本是对此无感知的</li><li>小盘换大盘，如果磁盘满了，需要添加磁盘，我们不需要将小盘中的数据移动到大盘，只需要为大盘添加一个软链接到小盘就可以。（注意一下权限和用户及用户主）<h4 id="查看hadoop目录"><a href="#查看hadoop目录" class="headerlink" title="查看hadoop目录"></a>查看hadoop目录</h4><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 app]</span>$ cd hadoop<span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ ll总用量 <span class="hljs-number">116</span>drwxr-xr-x.  <span class="hljs-number">2</span> hadoop hadoop   <span class="hljs-number">137</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> bindrwxr-xr-x.  <span class="hljs-number">2</span> hadoop hadoop   <span class="hljs-number">166</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> bin-mapreduce1drwxr-xr-x.  <span class="hljs-number">3</span> hadoop hadoop  <span class="hljs-number">4096</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> clouderadrwxr-xr-x.  <span class="hljs-number">6</span> hadoop hadoop   <span class="hljs-number">109</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> etcdrwxr-xr-x.  <span class="hljs-number">5</span> hadoop hadoop    <span class="hljs-number">43</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> examplesdrwxr-xr-x.  <span class="hljs-number">3</span> hadoop hadoop    <span class="hljs-number">28</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> examples-mapreduce1drwxr-xr-x.  <span class="hljs-number">2</span> hadoop hadoop   <span class="hljs-number">106</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> includedrwxr-xr-x.  <span class="hljs-number">3</span> hadoop hadoop    <span class="hljs-number">20</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> libdrwxr-xr-x.  <span class="hljs-number">3</span> hadoop hadoop   <span class="hljs-number">261</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> libexec-rw-r--r--.  <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">85063</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> LICENSE.txt-rw-r--r--.  <span class="hljs-number">1</span> hadoop hadoop <span class="hljs-number">14978</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> NOTICE.txt-rw-r--r--.  <span class="hljs-number">1</span> hadoop hadoop  <span class="hljs-number">1366</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> README.txtdrwxr-xr-x.  <span class="hljs-number">3</span> hadoop hadoop  <span class="hljs-number">4096</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> sbindrwxr-xr-x.  <span class="hljs-number">4</span> hadoop hadoop    <span class="hljs-number">31</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> sharedrwxr-xr-x. <span class="hljs-number">18</span> hadoop hadoop  <span class="hljs-number">4096</span> <span class="hljs-number">6</span>月   <span class="hljs-number">3</span> <span class="hljs-number">2019</span> src</code></pre>几个常用的目录：bin、sbin、etc</li><li>bin目录存放的是常用的命令</li><li>sbin目录存放的是集群启动关闭的脚本</li><li>etc存放的是配置文件<h4 id="修改hadoop-env-sh文件，显性java家目录"><a href="#修改hadoop-env-sh文件，显性java家目录" class="headerlink" title="修改hadoop-env.sh文件，显性java家目录"></a>修改hadoop-env.sh文件，显性java家目录</h4><pre><code class="hljs awk">[hadoop@hadoop100 hadoop]$ which java<span class="hljs-regexp">/usr/</span>java<span class="hljs-regexp">/jdk1.8.0_181/</span>bin/java[hadoop@hadoop100 hadoop]$ vim etc<span class="hljs-regexp">/hadoop/</span>hadoop-env.shexport JAVA_HOME=<span class="hljs-regexp">/usr/</span>java/jdk1.<span class="hljs-number">8.0</span>_181</code></pre><h4 id="配置hadoop用户的ssh信任关系"><a href="#配置hadoop用户的ssh信任关系" class="headerlink" title="配置hadoop用户的ssh信任关系"></a>配置hadoop用户的ssh信任关系</h4><pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 ~]</span>$ ssh-keygen  按三次回车<span class="hljs-string">[hadoop@hadoop100 ~]</span>$ ssh-copy-id hadoop100（这是我的主机名，更换成你的主机名）测试一下：<span class="hljs-string">[hadoop@hadoop100 ~]</span>$ ssh hadoop100Last login: Sun Nov <span class="hljs-number">22</span> <span class="hljs-number">13</span>:<span class="hljs-number">38</span>:<span class="hljs-number">51</span> <span class="hljs-number">2020</span> <span class="hljs-keyword">from</span> hadoop100<span class="hljs-string">[hadoop@hadoop100 ~]</span>$</code></pre><h4 id="配置core-site-xml文件"><a href="#配置core-site-xml文件" class="headerlink" title="配置core-site.xml文件"></a>配置core-site.xml文件</h4><pre><code class="hljs dts">[hadoop@hadoop100 hadoop]$ vim <span class="hljs-meta-keyword">/home/</span>hadoop<span class="hljs-meta-keyword">/app/</span>hadoop<span class="hljs-meta-keyword">/etc/</span>hadoop/core-site.xml<span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>fs.defaultFS<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>hdfs:<span class="hljs-comment">//hadoop100:9000&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>hadoop.tmp.dir<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-meta-keyword">/home/</span>hadoop/tmp<span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre></li><li>fs.defaultFS，配置的是namenode进程以hadoop100启动</li><li>hadoop.tmp.dir，配置的是hdfs数据的存放路径，默认是在/tmp目录中<br><strong>【坑】</strong><br>文件千万不要放在/tmp目录中，默认是30天清理一次<h4 id="配置hdfs-site-xml文件"><a href="#配置hdfs-site-xml文件" class="headerlink" title="配置hdfs-site.xml文件"></a>配置hdfs-site.xml文件</h4><pre><code class="hljs dts">[hadoop@hadoop100 hadoop]$ vim etc<span class="hljs-meta-keyword">/hadoop/</span>hdfs-site.xml<span class="hljs-params">&lt;configuration&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.replication<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-number">1</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>hadoop100:<span class="hljs-number">50090</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span>    <span class="hljs-params">&lt;property&gt;</span>        <span class="hljs-params">&lt;name&gt;</span>dfs.namenode.secondary.https-address<span class="hljs-params">&lt;/name&gt;</span>        <span class="hljs-params">&lt;value&gt;</span>hadoop100:<span class="hljs-number">50091</span><span class="hljs-params">&lt;/value&gt;</span>    <span class="hljs-params">&lt;/property&gt;</span><span class="hljs-params">&lt;/configuration&gt;</span></code></pre></li><li>dfs.replication配置的是副本数为1（默认是3）这里是伪分布式就设为1</li><li>dfs.namenode.secondary.http-address配置的是secondary namenode进程以hadoop100启动<h4 id="配置slaves文件"><a href="#配置slaves文件" class="headerlink" title="配置slaves文件"></a>配置slaves文件</h4><pre><code class="hljs awk">[hadoop@hadoop100 hadoop]$ vim etc<span class="hljs-regexp">/hadoop/</span>slaveshadoop100</code></pre></li><li>datanode进程以hadoop100启动</li><li>注意不能有空格和换行<h4 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h4><pre><code class="hljs awk">[hadoop@hadoop100 hadoop]$ cd <span class="hljs-regexp">/home/</span>hadoop<span class="hljs-regexp">/app/</span>hadoop[hadoop@hadoop100 hadoop]$ bin/hdfs namenode -format<span class="hljs-number">20</span><span class="hljs-regexp">/11/</span><span class="hljs-number">22</span> <span class="hljs-number">13</span>:<span class="hljs-number">57</span>:<span class="hljs-number">23</span> INFO common.Storage: Storage directory <span class="hljs-regexp">/home/</span>hadoop<span class="hljs-regexp">/tmp/</span>dfs/name has been successfully formatted. （表示格式化成功）</code></pre><h4 id="启动hdfs"><a href="#启动hdfs" class="headerlink" title="启动hdfs"></a>启动hdfs</h4><pre><code class="hljs elixir">[hadoop<span class="hljs-variable">@hadoop100</span> hadoop]<span class="hljs-variable">$ </span>sbin/start-dfs.sh</code></pre>使用jps命令查看进程<pre><code class="hljs angelscript"><span class="hljs-string">[hadoop@hadoop100 hadoop]</span>$ jps<span class="hljs-number">13776</span> Jps<span class="hljs-number">13297</span> NameNode<span class="hljs-number">13620</span> SecondaryNameNode<span class="hljs-number">13402</span> DataNode</code></pre>启动成功！</li><li>通过web界面访问：<br><a href="http://hadoop100:50070/explorer.html">http://hadoop100:50070/explorer.html</a><br>我在windows中配置了hadoop100的主机映射，如果没有配置，可以将hadoop100改成namenode进程启动的ip地址，我这里对应的就是hadoop100的地址：192.168.1.100<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><pre><code class="hljs awk">先上传一个文件到hdfs的/input目录下[hadoop@hadoop100 hadoop]$ bin<span class="hljs-regexp">/hadoop fs -put test.txt /i</span>nput运行一个测试程序[hadoop@hadoop100 hadoop]$ bin<span class="hljs-regexp">/hadoop jar share/</span>hadoop<span class="hljs-regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="hljs-number">2.6</span>.<span class="hljs-number">0</span>-cdh5.<span class="hljs-number">16.2</span>.jar wordcount <span class="hljs-regexp">/input /</span>output在/output中查看运行结果[hadoop@hadoop100 hadoop]$ bin<span class="hljs-regexp">/hdfs dfs -cat /</span>output/*hao<span class="hljs-number">1</span>ni<span class="hljs-number">2</span></code></pre>至此hdfs部署完成</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>hdfs</tag>
      
      <tag>hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hdfs架构</title>
    <link href="/2020/11/17/hdfs%E6%9E%B6%E6%9E%84/"/>
    <url>/2020/11/17/hdfs%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="HDFS主从架构"><a href="#HDFS主从架构" class="headerlink" title="HDFS主从架构"></a>HDFS主从架构</h4><h4 id="namenode：名称节点"><a href="#namenode：名称节点" class="headerlink" title="namenode：名称节点"></a>namenode：名称节点</h4><ul><li><p>存储文件的元数据信息<br>a. 文件的名称<br>b. 文件的目录结构<br>c. 文件的属性、权限、副本数、创建时间</p><p>d. 一个文件被对应切割哪些数据块（包含副本数的块），对应分布在哪些datanode</p><pre><code>blockmap块映射，namenode不会持久化存储这种映射关系，而是通过集群的启动和运行时datanode定期汇报blockreport给namenode，然后在内存中动态维护这种映射关系</code></pre></li><li><p>作用<br>管理文件系统的命名空间，也就是维护文件系统树的文件和文件夹</p><h4 id="secondary-namenode：第二名称节点"><a href="#secondary-namenode：第二名称节点" class="headerlink" title="secondary namenode：第二名称节点"></a>secondary namenode：第二名称节点</h4></li><li><p>作用<br>将namenode的fsimage和edits文件拿过来合并，备份，推送给namenode</p></li><li><p>checkpoint配置<br>dfs.namenode.checkpoint.period  3600<br>dfs.namenode.checkpoint.txns    1000000<br>SecondaryNameNode每隔一小时执行一次<br>一分钟检查一次操作次数，当操作次数达到1百万时，便会执行一次</p></li><li><p>合并流程<br> <img src="./images/1606379867364.png" alt="enter description here"></p></li><li><p>风险<br>早期为了解决nn是单点的，单点故障，增加一个snn，1小时的checkpoint虽然能够减轻单点故障的带来的数据丢失风险，但是如果在检查点到合并过程NN宕机仍然会丢失这部分的元数据，因此生产上不允许使用snn，是使用HA高可靠，通过配置另外一个实时备份的namenode节点，随时等待Active的namenode挂掉，然后自己成为Acitve</p><h4 id="datanode：数据节点"><a href="#datanode：数据节点" class="headerlink" title="datanode：数据节点"></a>datanode：数据节点</h4></li><li><p>存储数据块和数据块的校验和</p></li><li><p>每隔一定时间发送blockreport<br>dfs.blockreport.intervalMsec          21600000=6h<br>dfs.datanode.directoryscan.interval   21600=6h<br>数据块损坏后，在该节点执行directoryscan之前（dfs.datanode.directoryscan.interval决定），都不会发现损坏<br>在向namenode报告数据块信息之前（dfs.blockreport.intervalMsec决定），都不会恢复数据块，当namenode收到块信息后才会采取恢复措施</p></li><li><p>丢失或损坏块如何恢复（自动/手动）<br><a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p><h4 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h4><p><img src="./images/1606382544708.png" alt="enter description here"><br>对用户是无感知的</p></li></ul><ol><li><p> HDFS Client调用FileSystem.create(filePath)方法，去和NN进行【RPC】通信。NN会去check这个文件是否存在，是否有权限创建这个文件。假如都可以，就创建一个新的文件，但是这时没有数据，是不关联任何block的。NN根据文件的大小，根据块大小 副本数，计算要上传多少的块和对应哪些DN节点上。最终这个信息返回给客户端【FSDataOutputStream】对象</p></li><li><p>Client 调用客户端【FSDataOutputStream】对象的write方法，根据【副本放置策略】，将第一个块的第一个副本写到DN1，写完复制到DN2，写完再复制到DN3.当第三个副本写完，就返回一个ack package确认包给DN2,DN2接收到ack 加上自己写完，发送ack给DN1，DN1接收到ack加上自己写完，就发送ack给客户端【FSDataOutputStream】对象，告诉它第一个块三副本写完了。以此类推。</p></li><li><p>当所有的块全部写完，Client调用【FSDataOutputStream】对象的close方法，关闭输出流。再次调用FileSystem.complete方法 ，告诉nn文件写成功。</p></li></ol><p>伪分布式1台dn，副本数参数必须设置是1吗？<br>设置2 也可以写，显示丢失一个副本</p><p>生产上分布式  3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>可以写</p><p>生产上分布式  &gt;3台dn，副本数参数是3，如果其中一个dn挂了，数据是否能够写入？<br>肯定写</p><h4 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h4><p><img src="./images/1606382572787.png" alt="enter description here"></p><ol><li><p>Client调用FileSystem的open(filePath)，与NN进行【rpc】通信，返回该文件的部分或者全部的block列表也就是返回【FSDataIntputStream】对象</p></li><li><p>Client调度【FSDataIntputStream】对象的read方法，与第一个块的最近的DN的进行读取，读取完成后，会check，假如ok就关闭与DN通信。假如不ok，就会记录块+DN的信息，下次就不从这个节点读取。那么从第二个节点读取。假后与第二个块的最近的DN的进行读取，以此类推。假如当block的列表全部读取完成，文件还没结束，就调用FileSystem从NN获取下一批次的block列表。</p></li><li><p>Client调用【FSDataIntputStream】对象的close方法，关闭输入流。</p><h4 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h4></li></ol><ul><li>第一个副本：<br>放置在上传的DN节点上，就近原则，节省IO<br>假如非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点。</li><li>第二个副本:<br>放置在与第一个副本的不同机架上的某个节点</li><li>第三个副本:<br>与第二个副本放置同一个机架的不同节点上。</li><li>如果副本数设置更多，随机放。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>hdfs</tag>
      
      <tag>hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>timestamp和datetime</title>
    <link href="/2020/11/17/timestamp%E5%92%8Cdatetime/"/>
    <url>/2020/11/17/timestamp%E5%92%8Cdatetime/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><h4 id="两者都可以用来表示YYYY-MM-DD-HH-MM-SS类型的日期"><a href="#两者都可以用来表示YYYY-MM-DD-HH-MM-SS类型的日期" class="headerlink" title="两者都可以用来表示YYYY-MM-DD HH:MM:SS类型的日期"></a>两者都可以用来表示YYYY-MM-DD HH:MM:SS类型的日期</h4><h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><h4 id="两者的存储方式不一样"><a href="#两者的存储方式不一样" class="headerlink" title="两者的存储方式不一样"></a>两者的存储方式不一样</h4><p>timestamp是把客户端插入的时间从当前时区转化为UTC（世界标准时间）进行存储，查询时，将其又转化为客户端当前时区进行返回。<br>而对于datetime，不做任何改变，基本上是原样输入和输出。<br>验证如下：</p><pre><code class="hljs asciidoc">create table ruozedata.test1(id int, testtime timestamp);create table ruozedata.test2(id int, testtime datetime);insert into test1 values(1,<span class="hljs-emphasis">&#x27;20201117000000&#x27;</span>);insert into test2 values(1,<span class="hljs-emphasis">&#x27;20201117000000&#x27;</span>);mysql&gt; select * from test1;<span class="hljs-code">+------+</span>---------------------+| id   | testtime            |<span class="hljs-code">+------+</span>---------------------+|    1 | 2020-11-17 00:00:00 |<span class="hljs-code">+------+</span>---------------------+1 row in set (0.00 sec)mysql&gt; select * from test2;<span class="hljs-code">+------+</span>---------------------+| id   | testtime            |<span class="hljs-code">+------+</span>---------------------+|    1 | 2020-11-17 00:00:00 |<span class="hljs-code">+------+</span>---------------------+1 row in set (0.00 sec)</code></pre><p>可以看出二者的结果是一样的<br>现在修改当前会话的时区</p><pre><code class="hljs gherkin">mysql&gt; set time_zone=&#x27;+0:00&#x27;;mysql&gt; show variables like &#x27;%time_zone%&#x27;;+------------------+--------+|<span class="hljs-string"> Variable_name    </span>|<span class="hljs-string"> Value  </span>|+------------------+--------+|<span class="hljs-string"> system_time_zone </span>|<span class="hljs-string"> CST    </span>||<span class="hljs-string"> time_zone        </span>|<span class="hljs-string"> +00:00 </span>|+------------------+--------+</code></pre><p>修改后再次查询</p><pre><code class="hljs asciidoc">mysql&gt; select * from test1;<span class="hljs-code">+------+</span>---------------------+| id   | testtime            |<span class="hljs-code">+------+</span>---------------------+|    1 | 2020-11-16 16:00:00 |<span class="hljs-code">+------+</span>---------------------+1 row in set (0.00 sec)mysql&gt; select * from test2;<span class="hljs-code">+------+</span>---------------------+| id   | testtime            |<span class="hljs-code">+------+</span>---------------------+|    1 | 2020-11-17 00:00:00 |<span class="hljs-code">+------+</span>---------------------+1 row in set (0.00 sec)</code></pre><p>可以发现timestamp比datetime慢了8小时</p><h4 id="二者表示时间范围不同"><a href="#二者表示时间范围不同" class="headerlink" title="二者表示时间范围不同"></a>二者表示时间范围不同</h4><p>timestamp所能存储的时间范围为：’1970-01-01 00:00:01.000000’ 到 ‘2038-01-19 03:14:07.999999’。datetime所能存储的时间范围为：’1000-01-01 00:00:00.000000’ 到 ‘9999-12-31 23:59:59.999999’。<br>验证如下：</p><pre><code class="hljs pgsql">mysql&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test1 <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;15201117000000&#x27;</span>);ERROR <span class="hljs-number">1292</span> (<span class="hljs-number">22007</span>): Incorrect datetime <span class="hljs-keyword">value</span>: <span class="hljs-string">&#x27;15201117000000&#x27;</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">column</span> <span class="hljs-string">&#x27;testtime&#x27;</span> at <span class="hljs-keyword">row</span> <span class="hljs-number">1</span>mysql&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test2 <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;15201117000000&#x27;</span>);Query OK, <span class="hljs-number">1</span> <span class="hljs-keyword">row</span> affected (<span class="hljs-number">0.04</span> sec)mysql&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test1 <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;20391117000000&#x27;</span>);ERROR <span class="hljs-number">1292</span> (<span class="hljs-number">22007</span>): Incorrect datetime <span class="hljs-keyword">value</span>: <span class="hljs-string">&#x27;20391117000000&#x27;</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">column</span> <span class="hljs-string">&#x27;testtime&#x27;</span> at <span class="hljs-keyword">row</span> <span class="hljs-number">1</span>mysql&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test2 <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;20391117000000&#x27;</span>);Query OK, <span class="hljs-number">1</span> <span class="hljs-keyword">row</span> affected (<span class="hljs-number">0.00</span> sec)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL语法</title>
    <link href="/2020/11/15/SQL%E8%AF%AD%E6%B3%95/"/>
    <url>/2020/11/15/SQL%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h3 id="字段类型"><a href="#字段类型" class="headerlink" title="字段类型"></a>字段类型</h3><h4 id="数字类型"><a href="#数字类型" class="headerlink" title="数字类型"></a>数字类型</h4><p>int 整数<br>long 长整数<br>float 单精度<br>double 双精度<br>decimal 钱</p><h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><p>char 字节 定长0-255长度 qixxxxxx 会自动补齐<br>varchar 字符串 变长0-65535字节 qi</p><h4 id="日期"><a href="#日期" class="headerlink" title="日期"></a>日期</h4><p>date 日期 YYYY-MM-DD<br>time 时间 HH:MM:SS<br>datetime 年月日时分秒<br>timestamp 年月日时分秒<br>datetime与timestamp什么区别？</p><h3 id="sql类型"><a href="#sql类型" class="headerlink" title="sql类型"></a>sql类型</h3><h4 id="三种语言"><a href="#三种语言" class="headerlink" title="三种语言"></a>三种语言</h4><p>ddl 数据定义语言 create drop<br>dml 数据操作语言 select insert update delete （增删改查）<br>dcl 数据控制语言 grant </p><h3 id="建表规范"><a href="#建表规范" class="headerlink" title="建表规范"></a>建表规范</h3><pre><code class="hljs mysql">create table ruozedata.rz(id int(11)  not null auto_increment,name varchar(255) comment &#39;姓名&#39;,age  int(3) comment &#39;年龄&#39;,create_user varchar(255),create_time timestamp not null default current_timestamp,update_user varchar(255),update_time timestamp not null default current_timestamp on update current_timestamp,primary key(id));</code></pre><p>update_time timestamp not null default current_timestamp on update current_timestamp<br>这条语句（on语法）奠定了数据仓库的基础</p><h4 id="表名称、字段名称不要写中文，尽量不要汉语拼音"><a href="#表名称、字段名称不要写中文，尽量不要汉语拼音" class="headerlink" title="表名称、字段名称不要写中文，尽量不要汉语拼音"></a>表名称、字段名称不要写中文，尽量不要汉语拼音</h4><h4 id="统一风格"><a href="#统一风格" class="headerlink" title="统一风格"></a>统一风格</h4><h4 id="第一个字段必须是自增长字段，设置为主键且无业务意义"><a href="#第一个字段必须是自增长字段，设置为主键且无业务意义" class="headerlink" title="第一个字段必须是自增长字段，设置为主键且无业务意义"></a>第一个字段必须是自增长字段，设置为主键且无业务意义</h4><h4 id="一张表只有一个主键id，业务字段需要唯一的话，就使用唯一约束来保证"><a href="#一张表只有一个主键id，业务字段需要唯一的话，就使用唯一约束来保证" class="headerlink" title="一张表只有一个主键id，业务字段需要唯一的话，就使用唯一约束来保证"></a>一张表只有一个主键id，业务字段需要唯一的话，就使用唯一约束来保证</h4><h4 id="务必要加上crete-user、create-time、update-user、尤其是update-time"><a href="#务必要加上crete-user、create-time、update-user、尤其是update-time" class="headerlink" title="务必要加上crete_user、create_time、update_user、尤其是update_time"></a>务必要加上crete_user、create_time、update_user、尤其是update_time</h4><h4 id="业务字段，注释要加上-comment-‘xxx’"><a href="#业务字段，注释要加上-comment-‘xxx’" class="headerlink" title="业务字段，注释要加上 comment ‘xxx’"></a>业务字段，注释要加上 comment ‘xxx’</h4><h3 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h3><p>增：insert into 表(字段) values();<br>删：delete from 表 where 条件<br>改：update 表 set 字段= where 条件<br>差：select * from 表 where 字段</p><h3 id="常见的sql语法"><a href="#常见的sql语法" class="headerlink" title="常见的sql语法"></a>常见的sql语法</h3><h4 id="过滤-where"><a href="#过滤-where" class="headerlink" title="过滤 where"></a>过滤 where</h4><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz  <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span>=<span class="hljs-string">&#x27;ruoze&#x27;</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz  <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span>=<span class="hljs-string">&#x27;jepson&#x27;</span> <span class="hljs-keyword">and</span> age=<span class="hljs-number">22</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz  <span class="hljs-keyword">where</span> age=<span class="hljs-number">16</span> <span class="hljs-keyword">or</span> age=<span class="hljs-number">12</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz  <span class="hljs-keyword">where</span> age <span class="hljs-keyword">in</span> (<span class="hljs-number">16</span>,<span class="hljs-number">12</span>);<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz a <span class="hljs-keyword">where</span> <span class="hljs-keyword">exists</span> (<span class="hljs-keyword">select</span> b.id <span class="hljs-keyword">from</span> ruozedata.rz b <span class="hljs-keyword">where</span> (age=<span class="hljs-number">16</span> <span class="hljs-keyword">or</span> age=<span class="hljs-number">12</span> ) <span class="hljs-keyword">and</span> a.id=b.id );</code></pre><p>过滤可以使用 and or in等条件</p><h4 id="排序-order-by"><a href="#排序-order-by" class="headerlink" title="排序 order by"></a>排序 order by</h4><pre><code class="hljs sql">正序<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> age;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> age <span class="hljs-keyword">asc</span>;倒序<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> age <span class="hljs-keyword">desc</span>, <span class="hljs-keyword">name</span> <span class="hljs-keyword">desc</span>;</code></pre><p>asc、desc分别为正序、倒序，默认是正序</p><h4 id="模糊匹配-like"><a href="#模糊匹配-like" class="headerlink" title="模糊匹配 like"></a>模糊匹配 like</h4><pre><code class="hljs sql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;j%&#x27;</span>; 字母j开头<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%n&#x27;</span>; 字母n结尾<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%o%&#x27;</span>; 含有字母o<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">where</span> <span class="hljs-keyword">name</span> <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;__p%&#x27;</span>; 第三位字母为p</code></pre><p>%为任意字段，_ 表示为占位符</p><h4 id="合并表-union"><a href="#合并表-union" class="headerlink" title="合并表 union"></a>合并表 union</h4><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t1(<span class="hljs-keyword">id</span> <span class="hljs-built_in">int</span>,<span class="hljs-keyword">name</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">255</span>));<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t2(<span class="hljs-keyword">id</span> <span class="hljs-built_in">int</span>,<span class="hljs-keyword">name</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">255</span>));<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t1 <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;ruoze&#x27;</span>);<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t2 <span class="hljs-keyword">values</span>(<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;ruoze&#x27;</span>);<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t2 <span class="hljs-keyword">values</span>(<span class="hljs-number">2</span>,<span class="hljs-string">&#x27;jepson&#x27;</span>);加上all不会过滤掉重复的字段，<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t1<span class="hljs-keyword">union</span> <span class="hljs-keyword">all</span><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t2;不加all，会过滤重复的字段<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t1<span class="hljs-keyword">union</span><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t2;</code></pre><h4 id="分组语法-group-by-…-having"><a href="#分组语法-group-by-…-having" class="headerlink" title="分组语法 group by … having"></a>分组语法 group by … having</h4><pre><code class="hljs pgsql">分组/聚合函数sumavgmax/mincount不带分组字段的<span class="hljs-keyword">select</span> sum(age),avg(age),max(age),min(age),count(<span class="hljs-number">1</span>)<span class="hljs-keyword">from</span> ruozedata.rz;带分组字段的 <span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,sum(age),count(<span class="hljs-number">1</span>) <span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">having</span> sum(age)&gt;<span class="hljs-number">18</span>; ==&gt; 等价于子查询<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span>(<span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,sum(age) <span class="hljs-keyword">as</span> sum_age,count(<span class="hljs-number">1</span>) <span class="hljs-keyword">as</span> c<span class="hljs-keyword">from</span> ruozedata.rz <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> ) <span class="hljs-keyword">as</span> t <span class="hljs-keyword">where</span> t.sum_age&gt;<span class="hljs-number">18</span>;</code></pre><p>注：能用having就用having，实在不行就用子查询</p>]]></content>
    
    
    
    <tags>
      
      <tag>SQL语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux下配置jdk环境变量</title>
    <link href="/2020/11/08/Linux%E4%B8%8B%E9%85%8D%E7%BD%AEjdk%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    <url>/2020/11/08/Linux%E4%B8%8B%E9%85%8D%E7%BD%AEjdk%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h4 id="1、创建目录并解压tar包"><a href="#1、创建目录并解压tar包" class="headerlink" title="1、创建目录并解压tar包"></a>1、创建目录并解压tar包</h4><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># mkdir -p /usr/java</span>[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># tar -zxvf jdk-8u181-linux-x64.tar.gz -C /usr/java/</span></code></pre><h4 id="2、查看jdk"><a href="#2、查看jdk" class="headerlink" title="2、查看jdk"></a>2、查看jdk</h4><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># cd /usr/java/<span class="hljs-string">[root@hadoop100 java]</span># ll总用量 <span class="hljs-number">0</span>drwxr-xr-x. <span class="hljs-number">7</span> <span class="hljs-number">10</span> <span class="hljs-number">143</span> <span class="hljs-number">245</span> <span class="hljs-number">7</span>月   <span class="hljs-number">7</span> <span class="hljs-number">2018</span> jdk1<span class="hljs-number">.8</span><span class="hljs-number">.0</span>_181</code></pre><h4 id="3、更改文件的所有者、所属组"><a href="#3、更改文件的所有者、所属组" class="headerlink" title="3、更改文件的所有者、所属组"></a>3、更改文件的所有者、所属组</h4><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 java]</span># chown -R root:root jdk1<span class="hljs-number">.8</span><span class="hljs-number">.0</span>_181/<span class="hljs-string">[root@hadoop100 java]</span># lldrwxr-xr-x. <span class="hljs-number">7</span> root root <span class="hljs-number">245</span> <span class="hljs-number">7</span>月   <span class="hljs-number">7</span> <span class="hljs-number">2018</span> jdk1<span class="hljs-number">.8</span><span class="hljs-number">.0</span>_181</code></pre><p><strong>【注意】</strong><br>解压完jdk后，一定要查看一下jdk文件的所有者以及所属组，如果异常要更改！！！<br>例：drwxr-xr-x. 7 10 143 245 7月   7 2018 jdk1.8.0_181<br>这种情况，要更改文件的所有者所属组为root或者其他：<br>drwxr-xr-x. 7 root root 245 7月   7 2018 jdk1.8.0_181</p><h4 id="4、配置环境变量"><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h4><pre><code class="hljs routeros">[root@hadoop100 java]# vi /etc/profile<span class="hljs-comment"># 在尾行加入以下内容：</span><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_181<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$JAVA_HOME</span>/bin:$PATH</code></pre><h4 id="5、生效并查看环境变量"><a href="#5、生效并查看环境变量" class="headerlink" title="5、生效并查看环境变量"></a>5、生效并查看环境变量</h4><pre><code class="hljs routeros">[root@hadoop100 java]# source /etc/profile[root@hadoop100 java]# echo <span class="hljs-variable">$PATH</span>/usr/java/jdk1.8.0_181/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin[root@hadoop100 java]# java -versionjava version <span class="hljs-string">&quot;1.8.0_181&quot;</span>Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit<span class="hljs-built_in"> Server </span>VM (build 25.181-b13, mixed mode)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>jdk</tag>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL部署及常用命令</title>
    <link href="/2020/11/08/MySQL%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2020/11/08/MySQL%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="希望我的分享能够对大家有所帮助"><a href="#希望我的分享能够对大家有所帮助" class="headerlink" title="希望我的分享能够对大家有所帮助"></a><strong>希望我的分享能够对大家有所帮助</strong></h2><h3 id="MySQL部署"><a href="#MySQL部署" class="headerlink" title="MySQL部署"></a>MySQL部署</h3><h4 id="1、解压tar包，更名"><a href="#1、解压tar包，更名" class="headerlink" title="1、解压tar包，更名"></a>1、解压tar包，更名</h4><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># cd /usr/local/</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># tar zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># mv mysql-5.7.11-linux-glibc2.5-x86_64 mysql</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># mkdir mysql/arch mysql/data mysql/tmp</span></code></pre><h4 id="2、创建my-cnf文件"><a href="#2、创建my-cnf文件" class="headerlink" title="2、创建my.cnf文件"></a>2、创建my.cnf文件</h4><pre><code class="hljs nix">[root@hadoop100 local]<span class="hljs-comment"># vi /etc/my.cnf</span>dG清空文件所有内容添加以下内容：[client]<span class="hljs-attr">port</span>            = <span class="hljs-number">3306</span><span class="hljs-attr">socket</span>          = /usr/local/mysql/data/mysql.sock<span class="hljs-attr">default-character-set=utf8mb4</span>[mysqld]<span class="hljs-attr">port</span>            = <span class="hljs-number">3306</span><span class="hljs-attr">socket</span>          = /usr/local/mysql/data/mysql.sockskip-slave-startskip-external-locking<span class="hljs-attr">key_buffer_size</span> = <span class="hljs-number">256</span>M<span class="hljs-attr">sort_buffer_size</span> = <span class="hljs-number">2</span>M<span class="hljs-attr">read_buffer_size</span> = <span class="hljs-number">2</span>M<span class="hljs-attr">read_rnd_buffer_size</span> = <span class="hljs-number">4</span>M<span class="hljs-attr">query_cache_size=</span> <span class="hljs-number">32</span>M<span class="hljs-attr">max_allowed_packet</span> = <span class="hljs-number">16</span>M<span class="hljs-attr">myisam_sort_buffer_size=128M</span><span class="hljs-attr">tmp_table_size=32M</span><span class="hljs-attr">table_open_cache</span> = <span class="hljs-number">512</span><span class="hljs-attr">thread_cache_size</span> = <span class="hljs-number">8</span><span class="hljs-attr">wait_timeout</span> = <span class="hljs-number">86400</span><span class="hljs-attr">interactive_timeout</span> = <span class="hljs-number">86400</span><span class="hljs-attr">max_connections</span> = <span class="hljs-number">600</span><span class="hljs-comment"># Try number of CPU&#x27;s*2 for thread_concurrency</span><span class="hljs-comment">#thread_concurrency = 32 </span><span class="hljs-comment">#isolation level and default engine </span><span class="hljs-attr">default-storage-engine</span> = INNODB<span class="hljs-attr">transaction-isolation</span> = READ-COMMITTED<span class="hljs-attr">server-id</span>  = <span class="hljs-number">1739</span><span class="hljs-attr">basedir</span>     = /usr/local/mysql<span class="hljs-attr">datadir</span>     = /usr/local/mysql/data<span class="hljs-attr">pid-file</span>     = /usr/local/mysql/data/hostname.pid<span class="hljs-comment">#open performance schema</span>log-warningssysdate-is-now<span class="hljs-attr">binlog_format</span> = ROW<span class="hljs-attr">log_bin_trust_function_creators=1</span><span class="hljs-attr">log-error</span>  = /usr/local/mysql/data/hostname.err<span class="hljs-attr">log-bin</span> = /usr/local/mysql/arch/mysql-bin<span class="hljs-attr">expire_logs_days</span> = <span class="hljs-number">7</span><span class="hljs-attr">innodb_write_io_threads=16</span><span class="hljs-attr">relay-log</span>  = /usr/local/mysql/relay_log/relay-log<span class="hljs-attr">relay-log-index</span> = /usr/local/mysql/relay_log/relay-log.index<span class="hljs-attr">relay_log_info_file=</span> /usr/local/mysql/relay_log/relay-log.info<span class="hljs-attr">log_slave_updates=1</span><span class="hljs-attr">gtid_mode=OFF</span><span class="hljs-attr">enforce_gtid_consistency=OFF</span><span class="hljs-comment"># slave</span><span class="hljs-attr">slave-parallel-type=LOGICAL_CLOCK</span><span class="hljs-attr">slave-parallel-workers=4</span><span class="hljs-attr">master_info_repository=TABLE</span><span class="hljs-attr">relay_log_info_repository=TABLE</span><span class="hljs-attr">relay_log_recovery=ON</span><span class="hljs-comment">#other logs</span><span class="hljs-comment">#general_log =1</span><span class="hljs-comment">#general_log_file  = /usr/local/mysql/data/general_log.err</span><span class="hljs-comment">#slow_query_log=1</span><span class="hljs-comment">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span><span class="hljs-comment">#for replication slave</span><span class="hljs-attr">sync_binlog</span> = <span class="hljs-number">500</span><span class="hljs-comment">#for innodb options </span><span class="hljs-attr">innodb_data_home_dir</span> = /usr/local/mysql/data/<span class="hljs-attr">innodb_data_file_path</span> = ibdata1:<span class="hljs-number">1</span>G;ibdata2:<span class="hljs-number">1</span>G:autoextend<span class="hljs-attr">innodb_log_group_home_dir</span> = /usr/local/mysql/arch<span class="hljs-attr">innodb_log_files_in_group</span> = <span class="hljs-number">4</span><span class="hljs-attr">innodb_log_file_size</span> = <span class="hljs-number">1</span>G<span class="hljs-attr">innodb_log_buffer_size</span> = <span class="hljs-number">200</span>M<span class="hljs-comment">#根据生产需要，调整pool size </span><span class="hljs-attr">innodb_buffer_pool_size</span> = <span class="hljs-number">2</span>G<span class="hljs-comment">#innodb_additional_mem_pool_size = 50M #deprecated in 5.6</span><span class="hljs-attr">tmpdir</span> = /usr/local/mysql/tmp<span class="hljs-attr">innodb_lock_wait_timeout</span> = <span class="hljs-number">1000</span><span class="hljs-comment">#innodb_thread_concurrency = 0</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span> = <span class="hljs-number">2</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog=1</span><span class="hljs-comment">#innodb io features: add for mysql5.5.8</span>performance_schema<span class="hljs-attr">innodb_read_io_threads=4</span><span class="hljs-attr">innodb-write-io-threads=4</span><span class="hljs-attr">innodb-io-capacity=200</span><span class="hljs-comment">#purge threads change default(0) to 1 for purge</span><span class="hljs-attr">innodb_purge_threads=1</span><span class="hljs-attr">innodb_use_native_aio=on</span><span class="hljs-comment">#case-sensitive file names and separate tablespace</span><span class="hljs-attr">innodb_file_per_table</span> = <span class="hljs-number">1</span><span class="hljs-attr">lower_case_table_names=1</span>[mysqldump]quick<span class="hljs-attr">max_allowed_packet</span> = <span class="hljs-number">128</span>M[mysql]no-auto-rehash<span class="hljs-attr">default-character-set=utf8mb4</span>[mysqlhotcopy]interactive-timeout[myisamchk]<span class="hljs-attr">key_buffer_size</span> = <span class="hljs-number">256</span>M<span class="hljs-attr">sort_buffer_size</span> = <span class="hljs-number">256</span>M<span class="hljs-attr">read_buffer</span> = <span class="hljs-number">2</span>M<span class="hljs-attr">write_buffer</span> = <span class="hljs-number">2</span>M</code></pre><h4 id="3、创建用户组及用户"><a href="#3、创建用户组及用户" class="headerlink" title="3、创建用户组及用户"></a>3、创建用户组及用户</h4><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># groupadd -g 101 dba</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># useradd -u 514 -g dba -G root -d /usr/local/mysql/ mysqladmin</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># id mysqladmin</span>uid=<span class="hljs-number">514</span>(mysqladmin) gid=<span class="hljs-number">101</span>(dba) 组=<span class="hljs-number">101</span>(dba),<span class="hljs-number">0</span>(root)<span class="hljs-meta"># 一般不需要设置mysqladmin的密码，直接从root或者LDAP用户sudo切换</span></code></pre><h4 id="4-copy-环境变量配置文件至mysqladmin用户的home目录中"><a href="#4-copy-环境变量配置文件至mysqladmin用户的home目录中" class="headerlink" title="4.copy 环境变量配置文件至mysqladmin用户的home目录中"></a>4.copy 环境变量配置文件至mysqladmin用户的home目录中</h4><p>为了以下步骤配置个人环境变量，因为mysqladmin的家目录不在/home，样式会丢失，需要拷贝！！！！</p><pre><code class="hljs gradle">[root@hadoop100 local]# cp <span class="hljs-regexp">/etc/</span>skel<span class="hljs-regexp">/.* /u</span>sr<span class="hljs-regexp">/local/my</span>sql</code></pre><h4 id="5、配置环境变量"><a href="#5、配置环境变量" class="headerlink" title="5、配置环境变量"></a>5、配置环境变量</h4><pre><code class="hljs routeros">[root@hadoop100 local]# vi mysql/.bashrc添加以下内容：<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">MYSQL_BASE</span>=/usr/local/mysql<span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;MYSQL_BASE&#125;</span>/bin:$PATHunset USERNAME<span class="hljs-comment">#stty erase ^H</span><span class="hljs-builtin-name">set</span> umask <span class="hljs-keyword">to</span> 022umask 022<span class="hljs-attribute">PS1</span>=`uname -n`<span class="hljs-string">&quot;:&quot;</span><span class="hljs-string">&#x27;$USER&#x27;</span><span class="hljs-string">&quot;:&quot;</span><span class="hljs-string">&#x27;$PWD&#x27;</span><span class="hljs-string">&quot;:&gt;&quot;</span>; <span class="hljs-builtin-name">export</span> PS1<span class="hljs-comment">## end</span></code></pre><h4 id="6、赋权限和用户组，切换用户mysqladmin，安装"><a href="#6、赋权限和用户组，切换用户mysqladmin，安装" class="headerlink" title="6、赋权限和用户组，切换用户mysqladmin，安装"></a>6、赋权限和用户组，切换用户mysqladmin，安装</h4><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># chown mysqladmin:dba /etc/my.cnf</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># chmod 640 /etc/my.cnf</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># chown -R mysqladmin:dba /usr/local/mysql</span>[root<span class="hljs-symbol">@hadoop100</span> <span class="hljs-keyword">local</span>]<span class="hljs-meta"># chmod -R 755 /usr/local/mysql</span></code></pre><h4 id="7、配置服务及开机自启动"><a href="#7、配置服务及开机自启动" class="headerlink" title="7、配置服务及开机自启动"></a>7、配置服务及开机自启动</h4><pre><code class="hljs autoit"><span class="hljs-meta"># 将服务文件拷贝到init.d下，并重命名为mysql</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># cp support-files/mysql.server /etc/rc.d/init.d/mysql</span><span class="hljs-meta"># 赋予可执行权限</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># chmod +x /etc/rc.d/init.d/mysql </span><span class="hljs-meta"># 删除服务</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># chkconfig --del mysql</span><span class="hljs-meta"># 添加服务</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># chkconfig --add mysql</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># chkconfig --level 345 mysql on</span>[root<span class="hljs-symbol">@hadoop100</span> mysql]<span class="hljs-meta"># vi /etc/rc.local</span>添加以下内容：su - mysqladmin -c <span class="hljs-string">&quot;/etc/init.d/mysql start --federated&quot;</span></code></pre><h4 id="8、安装libaio及安装mysql的初始db"><a href="#8、安装libaio及安装mysql的初始db" class="headerlink" title="8、安装libaio及安装mysql的初始db"></a>8、安装libaio及安装mysql的初始db</h4><pre><code class="hljs sql">[root@hadoop100 ~]<span class="hljs-comment"># yum -y install libaio</span><span class="hljs-comment"># 如果遇到以下错误：</span>/var/run/yum.pid 已被锁定，PID 为 5357 的另一个程序正在运行。Another app is currently holding the yum <span class="hljs-keyword">lock</span>; waiting for it to exit...<span class="hljs-comment"># 解决方法：</span>[root@hadoop100 ~]<span class="hljs-comment"># rm -f /var/run/yum.pid</span><span class="hljs-comment"># 初始化：</span>bin/mysqld \<span class="hljs-comment">--defaults-file=/etc/my.cnf \</span><span class="hljs-comment">--user=mysqladmin \</span><span class="hljs-comment">--basedir=/usr/local/mysql/ \</span><span class="hljs-comment">--datadir=/usr/local/mysql/data/ \</span><span class="hljs-comment">--initialize</span><span class="hljs-comment"># 初始化时，可能会报以下错误：</span>[ERROR] Fatal error: Can&#x27;t <span class="hljs-keyword">change</span> <span class="hljs-keyword">to</span> run <span class="hljs-keyword">as</span> <span class="hljs-keyword">user</span> <span class="hljs-string">&#x27;mysql&#x27;</span> ;  Please <span class="hljs-keyword">check</span> that the <span class="hljs-keyword">user</span> <span class="hljs-keyword">exists</span>!<span class="hljs-comment"># 解决办法：删除目录中的data目录（个人是可以解决的）</span>hadoop100:mysqladmin:/usr/<span class="hljs-keyword">local</span>/mysql:&gt;rm -rf <span class="hljs-keyword">data</span>/</code></pre><h4 id="9、查看临时密码"><a href="#9、查看临时密码" class="headerlink" title="9、查看临时密码"></a>9、查看临时密码</h4><pre><code class="hljs less"><span class="hljs-attribute">hadoop100</span>:<span class="hljs-attribute">mysqladmin</span>:/usr/local/mysql/<span class="hljs-attribute">data</span>:&gt;cat hostname.err | grep password<span class="hljs-number">2020</span><span class="hljs-attribute">-11-16T05</span>:<span class="hljs-number">36</span>:<span class="hljs-number">10.531262</span>Z <span class="hljs-number">1</span> [Note] A temporary password is generated for root<span class="hljs-variable">@localhost</span>: wrZh5yitrF/_</code></pre><h4 id="10、启动"><a href="#10、启动" class="headerlink" title="10、启动"></a>10、启动</h4><pre><code class="hljs awk">hadoop100:mysqladmin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/mysql/</span>data:&gt;<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/mysql/</span>bin<span class="hljs-regexp">/mysqld_safe --defaults-file=/</span>etc/my.cnf &amp;<span class="hljs-comment"># 按两次回车</span></code></pre><h4 id="11、登录及初始化"><a href="#11、登录及初始化" class="headerlink" title="11、登录及初始化"></a>11、登录及初始化</h4><pre><code class="hljs routeros">hadoop100:mysqladmin:/usr/local/mysql/data:&gt;mysql -uroot -p<span class="hljs-string">&#x27;你的密码&#x27;</span>mysql: [<span class="hljs-builtin-name">Warning</span>] Using a password on the command line<span class="hljs-built_in"> interface </span>can be insecure.Welcome <span class="hljs-keyword">to</span> the MySQL monitor.  Commands end with ; <span class="hljs-keyword">or</span> \g.Your MySQL<span class="hljs-built_in"> connection </span>id is 2Server version: 5.7.11-logCopyright (c) 2000, 2016, Oracle <span class="hljs-keyword">and</span>/<span class="hljs-keyword">or</span> its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation <span class="hljs-keyword">and</span>/<span class="hljs-keyword">or</span> itsaffiliates. Other names may be trademarks of their respectiveowners.Type <span class="hljs-string">&#x27;help;&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;\h&#x27;</span> <span class="hljs-keyword">for</span> help.<span class="hljs-built_in"> Type </span><span class="hljs-string">&#x27;\c&#x27;</span> <span class="hljs-keyword">to</span> clear the current input statement.mysql&gt; <span class="hljs-comment"># 修改密码为‘ruozedata’</span>mysql&gt; alter<span class="hljs-built_in"> user </span>root@localhost identified by <span class="hljs-string">&#x27;ruozedata&#x27;</span>;Query OK, 0 rows affected (0.00 sec)mysql&gt; GRANT ALL PRIVILEGES ON *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">&#x27;root&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="hljs-string">&#x27;ruozedata&#x27;</span> ;Query OK, 0 rows affected, 1 <span class="hljs-builtin-name">warning</span> (0.00 sec)<span class="hljs-comment"># 刷新权限</span>mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)<span class="hljs-comment"># 退出</span>mysql&gt; exit;Bye<span class="hljs-comment"># 重启</span>hadoop100:mysqladmin:/usr/local/mysql/data:&gt;service mysql restarthadoop100:mysqladmin:/usr/local/mysql/data:&gt;mysql -uroot -pruozedata</code></pre><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><h4 id="1、创建用户并赋予权限"><a href="#1、创建用户并赋予权限" class="headerlink" title="1、创建用户并赋予权限"></a>1、创建用户并赋予权限</h4><p>创建database：ruozedata<br>create database ruozedata;<br>创建用户‘qi’，qi对于database：buozedata有所有的访问权限，用户密码是‘ruozedata’<br>grant all privileges on ruozedata.* to qi@’%’ identified by ‘ruozedata’;<br>刷新权限<br>flush privileges;<br><strong>【注意】</strong><br>对于mysql的用户操作，比如权限相关的 ，最后一步必须执行刷新权限。<br>%代表了 任意的客户端的IP地址 都被允许使用qi用户来远程访问</p><p>已经赋予%权限，在访问的时候还是有可能抛权限访问错误<br>这时可以尝试：<br>grant all privileges on ruozedata.* to qi@’客户端机器的IP’ identified by ‘ruozedata’;<br>如果这样配置：<br>grant all privileges on ruozedata.* to qi@’16.2.3.%’ identified by ‘ruozedata’;<br>16.2.3这个网段的所有ip都允许有权限去访问<br>16.2.3.1<br>16.2.3.2<br>16.2.3.3<br>….<br>16.2.3.255</p><pre><code class="hljs routeros">mysql&gt; select user,host, authentication_string <span class="hljs-keyword">from</span> user;+-----------+-----------+-------------------------------------------+|<span class="hljs-built_in"> user </span>     | host      | authentication_string                     |+-----------+-----------+-------------------------------------------+| root      | localhost | <span class="hljs-number">*4631F5CA72944B9DF43D706A6575ED330C93E3C8</span> || mysql.sys | localhost | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE || root      | %         | <span class="hljs-number">*4631F5CA72944B9DF43D706A6575ED330C93E3C8</span> || qi        | %         | <span class="hljs-number">*4631F5CA72944B9DF43D706A6575ED330C93E3C8</span> |+-----------+-----------+-------------------------------------------+4 rows <span class="hljs-keyword">in</span> <span class="hljs-builtin-name">set</span> (0.00 sec)</code></pre><h4 id="2、登录"><a href="#2、登录" class="headerlink" title="2、登录"></a>2、登录</h4><pre><code class="hljs awk"><span class="hljs-comment"># 有风险</span>hadoop100:mysqladmin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/mysql/</span>data:&gt;mysql -uqi -pruozedata -h192.<span class="hljs-number">168.1</span>.<span class="hljs-number">100</span></code></pre><p><strong>【推荐使用以下方式登录】</strong><br>mysql -uqi -p -h192.168.1.100，先不输入密码，等密码框弹出来后在输入<br>如果用第一种方法登录，可以在使用history查看到密码</p>]]></content>
    
    
    
    <tags>
      
      <tag>常用命令</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux实用命令及注意事项2</title>
    <link href="/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B92/"/>
    <url>/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B92/</url>
    
    <content type="html"><![CDATA[<h2 id="欢迎来到onlyroads，希望以下内容能对你有所帮助！"><a href="#欢迎来到onlyroads，希望以下内容能对你有所帮助！" class="headerlink" title=" 欢迎来到onlyroads，希望以下内容能对你有所帮助！"></a> 欢迎来到onlyroads，希望以下内容能对你有所帮助！</h2><h4 id="rm-rf【高危命令】"><a href="#rm-rf【高危命令】" class="headerlink" title="rm -rf【高危命令】"></a>rm -rf【高危命令】</h4><p>rm -rf / 直接删除根目录的所有内容，且不提示<br>rm xxx.log 会有提示<br>rm -r ydir 删除目录，且有提示，-f不提示</p><p>业务逻辑判断 赋值的 LOG_PATH=/xxx/yyy<br>如果漏了一个变量没有赋值，那么脚本中rm -rf ${LOG}/* 就等于是rm -rf /*  很危险<br>如何避免：<br>set -u参数 写在脚本的第二行，第一行是#!/bin/bash<br>如果变量在未复制的情况在会抛错</p><h4 id="用户与用户组"><a href="#用户与用户组" class="headerlink" title="用户与用户组"></a>用户与用户组</h4><h5 id="查看相关常用命令"><a href="#查看相关常用命令" class="headerlink" title="查看相关常用命令"></a>查看相关常用命令</h5><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># ll /usr/sbin/user*-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">118192</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/useradd-rwxr-x---. <span class="hljs-number">1</span> root root  <span class="hljs-number">80360</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/userdel-rws--x--x. <span class="hljs-number">1</span> root root  <span class="hljs-number">40312</span> <span class="hljs-number">6</span>月  <span class="hljs-number">10</span> <span class="hljs-number">2014</span> /usr/sbin/userhelper-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">113840</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/usermod-rwsr-xr-x. <span class="hljs-number">1</span> root root  <span class="hljs-number">11288</span> <span class="hljs-number">4</span>月  <span class="hljs-number">11</span> <span class="hljs-number">2018</span> /usr/sbin/usernetctl<span class="hljs-string">[root@hadoop100 ~]</span># ll /usr/sbin/group*-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">65480</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/groupadd-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">57016</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/groupdel-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">57064</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/groupmems-rwxr-x---. <span class="hljs-number">1</span> root root <span class="hljs-number">76424</span> <span class="hljs-number">11</span>月  <span class="hljs-number">6</span> <span class="hljs-number">2016</span> /usr/sbin/groupmod</code></pre><h5 id="创建用户的同时默认也会创建一个同名的组"><a href="#创建用户的同时默认也会创建一个同名的组" class="headerlink" title="创建用户的同时默认也会创建一个同名的组"></a>创建用户的同时默认也会创建一个同名的组</h5><pre><code class="hljs ruby">[root@hadoop100 ~]<span class="hljs-comment"># useradd yy</span>[root@hadoop100 ~]<span class="hljs-comment"># id</span>uid=<span class="hljs-number">0</span>(root) gid=<span class="hljs-number">0</span>(root) 组=<span class="hljs-number">0</span>(root) 环境=<span class="hljs-symbol">unconfined_u:</span><span class="hljs-symbol">unconfined_r:</span><span class="hljs-symbol">unconfined_t:</span>s<span class="hljs-number">0</span>-<span class="hljs-symbol">s0:</span>c<span class="hljs-number">0</span>.c1023[root@hadoop100 ~]<span class="hljs-comment"># cat /etc/passwd 查看用户信息</span> <span class="hljs-symbol">yy:</span><span class="hljs-symbol">x:</span><span class="hljs-number">1006</span><span class="hljs-symbol">:</span><span class="hljs-number">1006</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:/home/yy</span><span class="hljs-symbol">:/bin/bash</span>[root@hadoop100 ~]<span class="hljs-comment"># cat /etc/group 查看组信息</span><span class="hljs-symbol">yy:</span><span class="hljs-symbol">x:</span><span class="hljs-number">1006</span><span class="hljs-symbol">:</span></code></pre><h5 id="切换与删除用户"><a href="#切换与删除用户" class="headerlink" title="切换与删除用户"></a>切换与删除用户</h5><p>su - yy 切换后会回到yy用户的家目录<br>su yy 切换后还是在当前目录</p><pre><code class="hljs awk">[root@hadoop100 ~]<span class="hljs-comment"># su - yy</span>[yy@hadoop100 ~]$ pwd<span class="hljs-regexp">/home/yy</span>[yy@hadoop100 ~]$ <span class="hljs-keyword">exit</span>登出[root@hadoop100 ~]<span class="hljs-comment"># su yy</span>[yy@hadoop100 root]$ pwd/root</code></pre><p>userdel 删除用户，如果该用户的用户组没有其他用户则一起删除<br>但仅仅删除/etc/passwd中关于用户的内容，并不会删除家目录中的内容</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># userdel yy<span class="hljs-string">[root@hadoop100 ~]</span># ls -l /home/drwx------.  <span class="hljs-number">5</span>      <span class="hljs-number">1006</span>    <span class="hljs-number">1006</span>  <span class="hljs-number">128</span> <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">09</span>:<span class="hljs-number">01</span> yy</code></pre><h5 id="样式丢失"><a href="#样式丢失" class="headerlink" title="样式丢失"></a>样式丢失</h5><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 home]</span># useradd xy<span class="hljs-string">[root@hadoop100 home]</span># rm -rf /home/xy/.bash*一个用户默认的家目录是在/home，可以通过usermod -d 更改<span class="hljs-string">[root@hadoop100 home]</span># usermod -d /tmp xy<span class="hljs-string">[root@hadoop100 home]</span># su - xy上一次登录：一 <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">09</span>:<span class="hljs-number">28</span>:<span class="hljs-number">35</span> CST <span class="hljs-number">2020</span>pts/<span class="hljs-number">0</span> 上-bash<span class="hljs-number">-4.2</span>$</code></pre><p>我们发现 [root@hadoop100 home]这种样式丢失了，变成了-bash-4.2$ 这种样式<br>原因就是当前用户的所属家目录的个人环境变量文件不存在<br>在/ec/skel/文件夹中有个人环境变量的文件</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># ll -a /etc/skel/总用量 <span class="hljs-number">24</span>drwxr-xr-x.   <span class="hljs-number">3</span> root root   <span class="hljs-number">78</span> <span class="hljs-number">4</span>月  <span class="hljs-number">11</span> <span class="hljs-number">2018</span> .drwxr-xr-x. <span class="hljs-number">141</span> root root <span class="hljs-number">8192</span> <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">10</span>:<span class="hljs-number">59</span> ..-rw-r--r--.   <span class="hljs-number">1</span> root root   <span class="hljs-number">18</span> <span class="hljs-number">4</span>月  <span class="hljs-number">11</span> <span class="hljs-number">2018</span> .bash_logout-rw-r--r--.   <span class="hljs-number">1</span> root root  <span class="hljs-number">193</span> <span class="hljs-number">4</span>月  <span class="hljs-number">11</span> <span class="hljs-number">2018</span> .bash_profile-rw-r--r--.   <span class="hljs-number">1</span> root root  <span class="hljs-number">231</span> <span class="hljs-number">4</span>月  <span class="hljs-number">11</span> <span class="hljs-number">2018</span> .bashrcdrwxr-xr-x.   <span class="hljs-number">4</span> root root   <span class="hljs-number">39</span> <span class="hljs-number">8</span>月  <span class="hljs-number">10</span> <span class="hljs-number">22</span>:<span class="hljs-number">48</span> .mozilla</code></pre><p>我们只需复制到用户家目录中即可</p><pre><code class="hljs awk">cp <span class="hljs-regexp">/etc/</span>skel<span class="hljs-regexp">/.* /</span>home<span class="hljs-regexp">/xx/</span>[root@hadoop100 home]<span class="hljs-comment"># su xy</span>[xy@hadoop100 home]$</code></pre><p>这样样式就恢复了</p><h5 id="添加用户到新的用户组"><a href="#添加用户到新的用户组" class="headerlink" title="添加用户到新的用户组"></a>添加用户到新的用户组</h5><p>一个用户可以存在于多个组<br>以下代码就是将 xy添加到 另一个dw组中，并且让主组gid=dw，其他组为xy<br>这样，xy 就同时属于 dw组和xy组</p><pre><code class="hljs ini"><span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># groupadd dw</span><span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># usermod -g dw xy</span><span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># id xy</span><span class="hljs-attr">uid</span>=<span class="hljs-number">1007</span>(xy) gid=<span class="hljs-number">1008</span>(dw) 组=<span class="hljs-number">1008</span>(dw)<span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># usermod -a -G xy xy</span><span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># id xy</span><span class="hljs-attr">uid</span>=<span class="hljs-number">1007</span>(xy) gid=<span class="hljs-number">1008</span>(dw) 组=<span class="hljs-number">1008</span>(dw),<span class="hljs-number">1007</span>(xy)<span class="hljs-section">[root@hadoop100 home]</span><span class="hljs-comment"># </span></code></pre><h5 id="passwd文件"><a href="#passwd文件" class="headerlink" title="passwd文件"></a>passwd文件</h5><p>查看 /etc/passwd文件会发现结尾有两种 /bin/bash，/sbin/nologin</p><pre><code class="hljs ruby">[root@hadoop100 ~]<span class="hljs-comment"># cat /etc/passwd</span><span class="hljs-symbol">postfix:</span><span class="hljs-symbol">x:</span><span class="hljs-number">89</span><span class="hljs-symbol">:</span><span class="hljs-number">89</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:/var/spool/postfix</span><span class="hljs-symbol">:/sbin/nologin</span><span class="hljs-symbol">xy:</span><span class="hljs-symbol">x:</span><span class="hljs-number">1007</span><span class="hljs-symbol">:</span><span class="hljs-number">1008</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:/home/xy</span><span class="hljs-symbol">:/bin/bash</span></code></pre><p>进入 /etc/passwd 文件将xy用户的/bin/bash改为 /sbin/nologin</p><pre><code class="hljs ruby"><span class="hljs-symbol">xy:</span><span class="hljs-symbol">x:</span><span class="hljs-number">1007</span><span class="hljs-symbol">:</span><span class="hljs-number">1008</span><span class="hljs-symbol">:</span><span class="hljs-symbol">:/home/xy</span><span class="hljs-symbol">:/sbin/nologin</span></code></pre><p>然后切换到 xy 用户，发现无法切换</p><pre><code class="hljs coffeescript">[root@hadoop100 ~]<span class="hljs-comment"># su xy</span>This account <span class="hljs-keyword">is</span> currently <span class="hljs-keyword">not</span> available.</code></pre><p>我们再将xy用户的 /sbin/nologin 改为 /bin/false<br>然后进行切换</p><pre><code class="hljs less"><span class="hljs-attribute">xy</span>:<span class="hljs-attribute">x</span>:<span class="hljs-number">1007</span>:<span class="hljs-number">1008</span>::/home/<span class="hljs-attribute">xy</span>:/bin/false[root<span class="hljs-variable">@hadoop100</span> ~]# su - xy上一次登录：一 <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">11</span>:<span class="hljs-number">33</span>:<span class="hljs-number">15</span> CST <span class="hljs-number">2020pt</span>s/<span class="hljs-number">0</span> 上[root<span class="hljs-variable">@hadoop100</span> ~]#</code></pre><p>这时候我们发现登不进去，但不会出现 This account is currently not available.</p><p>CDH集群部署，hdfs hbase等等用户，su - hdfs切不过去的，原因就在  /bin/false<br>jps命令查看都是 不可用的<br>su - hdfs 之前 将/bin/false 改为 /bin/bash 即可切换成功。</p><h5 id="sudo命令"><a href="#sudo命令" class="headerlink" title="sudo命令"></a>sudo命令</h5><p>su xx 切换用户<br>su - xx 切换用户 进入家目录 且执行该用户环境变量文件 .bashrc<br>sudo: 普通用户临时使用root的最大权限</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># su - xy上一次登录：一 <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">11</span>:<span class="hljs-number">33</span>:<span class="hljs-number">22</span> CST <span class="hljs-number">2020</span>pts/<span class="hljs-number">0</span> 上<span class="hljs-string">[xy@hadoop100 ~]</span>$ cat /root/a.txtcat: /root/a.txt: 权限不够</code></pre><p>在 /etc/sudoers文件中添加<br>xy      ALL=(ALL)       ALL</p><pre><code class="hljs gams">vim /etc/sudoersroot    <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)       <span class="hljs-keyword">ALL</span>xy      <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)       <span class="hljs-keyword">ALL</span>[xy@hadoop100 ~]<span class="hljs-symbol">$</span> sudo cat /root/a.txtabc</code></pre><h4 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h4><pre><code class="hljs angelscript">drwxr-xr-x. <span class="hljs-number">2</span> root root   <span class="hljs-number">6</span> <span class="hljs-number">11</span>月  <span class="hljs-number">8</span> <span class="hljs-number">21</span>:<span class="hljs-number">58</span> a-rw-r--r--. <span class="hljs-number">1</span> root root <span class="hljs-number">136</span> <span class="hljs-number">11</span>月  <span class="hljs-number">7</span> <span class="hljs-number">23</span>:<span class="hljs-number">33</span> bb.txt-rw-r--r--. <span class="hljs-number">1</span> root root   <span class="hljs-number">9</span> <span class="hljs-number">11</span>月  <span class="hljs-number">7</span> <span class="hljs-number">23</span>:<span class="hljs-number">36</span> b.txt</code></pre><p>d rwx r-x r-x<br>第一个字符 d 代表文件夹 - 代表文件<br>rwx r-x r-x<br>三组的三个字母 分别是 读r 4、写w 2、 执x 1、-没有任何权限 0<br>第一组：rwx 文件或者文件夹的所属用户，有读写执行权限，<br>第二组：r-x 文件或者文件夹所属用户组的成员，只有读执行的权限<br>第三组：r-x 其他用户组的成员对这个文件或文件夹的权限，只有读和执行</p><p>权限的数字表示<br>777  rwxrwxrwx<br>755  rwxr-xr-x<br>640  rw-r—–<br>600  rw——-</p><p>权限命令<br>chmod -R 777 文件或文件夹，更改文件的权限<br>chown -R 用户:用户组 文件或文件夹，更改文件的用户和所属组</p><p>举一个小例子，分析一下</p><pre><code class="hljs angelscript"><span class="hljs-string">[xy@hadoop100 ~]</span>$ ll总用量 <span class="hljs-number">4</span>-rw-r--r--. <span class="hljs-number">1</span> xy dw <span class="hljs-number">2</span> <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">11</span>:<span class="hljs-number">57</span> a.txt<span class="hljs-string">[xy@hadoop100 ~]</span>$ chmod <span class="hljs-number">244</span> a.txt<span class="hljs-string">[xy@hadoop100 ~]</span>$ ll总用量 <span class="hljs-number">4</span>--w-r--r--. <span class="hljs-number">1</span> xy dw <span class="hljs-number">2</span> <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">11</span>:<span class="hljs-number">57</span> a.txt<span class="hljs-string">[xy@hadoop100 ~]</span>$ cat a.txt cat: a.txt: 权限不够</code></pre><p>为什么会出现权限不够？<br>因为我用chmod 244 命令将a.txt文件的所有者的权限改为–w<br>即对于a.txt文件的所有者来说只能写，不能读<br>如果想要查看，给文件所有者添加写权限几个</p><pre><code class="hljs angelscript"><span class="hljs-string">[xy@hadoop100 ~]</span>$ chmod <span class="hljs-number">644</span> a.txt <span class="hljs-string">[xy@hadoop100 ~]</span>$ ll总用量 <span class="hljs-number">4</span>-rw-r--r--. <span class="hljs-number">1</span> xy dw <span class="hljs-number">2</span> <span class="hljs-number">11</span>月  <span class="hljs-number">9</span> <span class="hljs-number">11</span>:<span class="hljs-number">57</span> a.txt<span class="hljs-string">[xy@hadoop100 ~]</span>$ cat a.txt a</code></pre><p>以后遇到权限问题，先要清楚是文件的所有者、所属组还是其他组的用户访问失败<br>然后再对相应用户添加相应的权限即可</p><h4 id="大小"><a href="#大小" class="headerlink" title="大小"></a>大小</h4><p>文件大小 ll -h<br>文件夹大小 du -sh<br>查看/home文件夹中的大小</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># du -sh /home<span class="hljs-number">221</span>M/home</code></pre><h4 id="搜索-find"><a href="#搜索-find" class="headerlink" title="搜索 find"></a>搜索 find</h4><p>find file -name ‘xxx’</p><pre><code class="hljs gradle">[root@hadoop100 atguigu]# <span class="hljs-keyword">find</span> <span class="hljs-regexp">/opt/m</span>odule/ -name <span class="hljs-string">&#x27;*hadoop*&#x27;</span><span class="hljs-regexp">/opt/m</span>odule/hadoop-<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span><span class="hljs-regexp">/opt/m</span>odule<span class="hljs-regexp">/hadoop-2.7.2/</span>etc/hadoop</code></pre><p>history 去看看常规操作<br>find 命令搜索<br>ps -ef 查看进程</p><h3 id="vi-编辑"><a href="#vi-编辑" class="headerlink" title="vi 编辑"></a>vi 编辑</h3><p>良好习惯：<br>vi 编辑配置文件<br>先 cp conf conf20201109<br>vi 修改完并wq保存后，如果发现改错文件了，也不怕，有备份</p><p>有时vim进入文件后会出现以下内容：</p><pre><code class="hljs vim">E325: 注意发现交换文件 <span class="hljs-string">&quot;.b.txt.swp&quot;</span>            所有者: root    日期: Mon Nov  <span class="hljs-number">9</span> <span class="hljs-number">12</span>:<span class="hljs-number">11</span>:<span class="hljs-number">24</span> <span class="hljs-number">2020</span>            文件名: ~root/<span class="hljs-keyword">b</span>.txt            修改过: 否            用户名: root      主机名: hadoop100           进程 ID: <span class="hljs-number">3551</span> (仍在运行)正在打开文件 <span class="hljs-string">&quot;b.txt&quot;</span>(<span class="hljs-number">1</span>) Another program may <span class="hljs-keyword">be</span> editing the same <span class="hljs-keyword">file</span>.  If this <span class="hljs-keyword">is</span> the case,    <span class="hljs-keyword">be</span> careful not <span class="hljs-keyword">to</span> end <span class="hljs-keyword">up</span> with two different instances of the same    <span class="hljs-keyword">file</span> when making <span class="hljs-keyword">changes</span>.  Quit, <span class="hljs-built_in">or</span> <span class="hljs-keyword">continue</span> with caution.(<span class="hljs-number">2</span>) An <span class="hljs-keyword">edit</span> session <span class="hljs-keyword">for</span> this <span class="hljs-keyword">file</span> crashed.    如果是这样，请用 <span class="hljs-string">&quot;:recover&quot;</span> 或 <span class="hljs-string">&quot;vim -r b.txt&quot;</span>    恢复修改的内容 (请见 <span class="hljs-string">&quot;:help recovery&quot;</span>)。    如果你已经进行了恢复，请删除交换文件 <span class="hljs-string">&quot;.b.txt.swp&quot;</span>    以避免再看到此消息。交换文件 <span class="hljs-string">&quot;.b.txt.swp&quot;</span> 已存在！</code></pre><p>进入提示是 swap文件，只需ll -a查看，进行删除即可</p><pre><code class="hljs css"><span class="hljs-selector-attr">[root@hadoop100 ~]</span># <span class="hljs-selector-tag">ll</span> <span class="hljs-selector-tag">-a</span><span class="hljs-selector-tag">-rw-------</span>.  1 <span class="hljs-selector-tag">root</span> <span class="hljs-selector-tag">root</span>  4096 11月  9 12<span class="hljs-selector-pseudo">:11</span> <span class="hljs-selector-class">.b</span><span class="hljs-selector-class">.txt</span><span class="hljs-selector-class">.swp</span><span class="hljs-selector-attr">[root@hadoop100 ~]</span># <span class="hljs-selector-tag">rm</span> <span class="hljs-selector-tag">-rf</span> <span class="hljs-selector-class">.b</span><span class="hljs-selector-class">.txt</span><span class="hljs-selector-class">.swp</span></code></pre><p><strong>【粘贴的坑】</strong><br>必须按 i 进入编辑模式，否则第一行内容丢失，不完成【生产巨坑】</p><p>在命令行模式，常用的快捷方式<br>dd 删除当前行<br>dG 删除当前行及以下所有行<br>ndd 删除当前行及以下n-1行</p><p>gg 跳转到第一行的第一个字母<br>G 跳转到最后一行的第一个字母<br>shift+$ 行尾<br>如何通过vi命令清空文件内容：gg  dG</p><p>显示行号：set nu<br>不显示行号：set nonu</p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>相应的坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux实用命令及注意事项</title>
    <link href="/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
    <url>/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    
    <content type="html"><![CDATA[<h3 id="欢迎来到onlyroads，希望我的分享能够对大家有所帮助"><a href="#欢迎来到onlyroads，希望我的分享能够对大家有所帮助" class="headerlink" title="欢迎来到onlyroads，希望我的分享能够对大家有所帮助"></a>欢迎来到onlyroads，希望我的分享能够对大家有所帮助</h3><hr><h4 id="mv和cp"><a href="#mv和cp" class="headerlink" title="mv和cp"></a>mv和cp</h4><p>mv：移动<br>&nbsp;&nbsp;&nbsp;&nbsp;在相同目录下相当于重命名 eg：mv a b<br>&nbsp;&nbsp;&nbsp;&nbsp;在不同目录下为移动 eg:mv a /tmp/a  标准操作<br>&nbsp;&nbsp;&nbsp;&nbsp;在不同目录下为移动 eg:mv a /tmp/  非标准操作<br>cp：复制<br>&nbsp;&nbsp;&nbsp;&nbsp; cp -r a /tmp/a  &nbsp;标准操作<br>&nbsp;&nbsp;&nbsp;&nbsp; cp -r a /tmp &nbsp;&nbsp;&nbsp; 非标准操作<br>&nbsp;&nbsp;&nbsp;&nbsp; cp -r a /tmp/aa  复制的同时修改名称</p><h4 id="创建文件"><a href="#创建文件" class="headerlink" title="创建文件"></a>创建文件</h4><p>创建一个空文件<br>&nbsp;&nbsp;&nbsp;&nbsp; touch a.txt（推荐）<br>&nbsp;&nbsp;&nbsp;&nbsp; vim a.txt（麻烦）<br>&nbsp;&nbsp;&nbsp;&nbsp; 默认进入命令行模式<br>&nbsp;&nbsp;&nbsp;&nbsp; i键&nbsp;&nbsp;&nbsp;进入命令行模式–》编辑模式<br>&nbsp;&nbsp;&nbsp;&nbsp; esc键&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从编辑模式–》命令行模式<br>&nbsp;&nbsp;&nbsp;&nbsp; shift+: 从命令行模式–》尾行模式$nbsp;&nbsp;wq报错退出, q!不保存退出<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cat /dev/null &gt; a.txt   清空文件内容为【推荐使用】<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>【坑！！】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;echo “” &gt; a.txt 【慎用】执行该命令后a.txt的内容不为空，占一个字节<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如在如下场景：先清理内容，再判断是否为空，假如为空就继续执行本内容否则退出 </p><h4 id="gt-和-gt-gt"><a href="#gt-和-gt-gt" class="headerlink" title="&gt; 和 &gt;&gt;"></a>&gt; 和 &gt;&gt;</h4><p>&gt; 将文本内容 覆盖掉   【高危命令1】<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cp mysql.cnf  mysql.cnf20201107  养成备份的习惯，很重要<br>&gt;&gt; 将文本内容 追加</p><h4 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h4><p>cat 文件内容一下子全部显示  ctrl+z中断    【推荐】<br>    &nbsp;&nbsp;&nbsp;&nbsp;适合内容少的，内容多 不适合</p><p>more 文件内容一页页 往下翻，按空格往下翻，ctrl+b回退  q退出 【推荐】<br>        &nbsp;&nbsp;&nbsp;&nbsp;适合内容多的 </p><p>less 文件内容 按键盘的上下键  按行为单位  q退出<br>tail 实时查看文件内容<br>&nbsp;&nbsp;&nbsp;&nbsp;-f   假如文件被移除 然后重命名 就无法再监控到文件<br>&nbsp;&nbsp;&nbsp;&nbsp;-F   假如文件被移除 然后重命名 会不断的retry尝试 去监控文件，直到监控到位<br>&nbsp;&nbsp;&nbsp;&nbsp;想要查看文件内容倒数100行，且实时监控：tail -100f 1.log</p><p>grep 和 |<br>&nbsp;&nbsp;&nbsp;&nbsp;log日志内容特别多 如何快速定位到ERROR、关键词信息<br>&nbsp;&nbsp;&nbsp;&nbsp;cat CloudAgent.log | grep ERROR<br>&nbsp;&nbsp;&nbsp;&nbsp;cat CloudAgent.log | grep -A 10 ERROR 后10行<br>&nbsp;&nbsp;&nbsp;&nbsp;cat CloudAgent.log | grep -B 10 ERROR 前10行<br>&nbsp;&nbsp;&nbsp;&nbsp;cat CloudAgent.log | grep -C 10 ERROR 前后各10行 20行 【推荐】<br>&nbsp;&nbsp;&nbsp;&nbsp;如果ERROR 非常非常多   【推荐使用以下操作】<br>&nbsp;&nbsp;&nbsp;&nbsp;cat manyerrors.log | grep -C 20 ERROR &gt; error.log<br>&nbsp;&nbsp;&nbsp;&nbsp;more error.log</p><p>&nbsp;&nbsp;&nbsp;&nbsp;将日志文件 下载到window电脑，进行搜索 定位 分析  【推荐】<br>&nbsp;&nbsp;<strong>【坑！！】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;假如CloudAgent.log 原文件很大，那么从生产下载到公司网络要走外网带宽大概10M，会影响公司服务<br>&nbsp;&nbsp;&nbsp;&nbsp;建议： 假如下载大文件，业务高峰或者工作日白天尽量不要做，或者进行【限速】（FTP可设置）</p><p>&nbsp;&nbsp;&nbsp;&nbsp;log4j日志组件<br>&nbsp;&nbsp;&nbsp;&nbsp;规则：每个log日志文件大小100M，保留10份<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mv erp.log erp.log1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;touch erp.log<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log 再由空的写满到100M<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log1 100m<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mv erp.log erp.log2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;touch erp.log<br>&nbsp;&nbsp;&nbsp;&nbsp;以此类推……<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log 空的<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log1 100M<br>&nbsp;&nbsp;&nbsp;&nbsp;erp.log2 100M<br>&nbsp;&nbsp;&nbsp;&nbsp;…….</p><p>&nbsp;&nbsp;<strong>【坑！！】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;对于大数据来说使用flume对日志文件采集<br>&nbsp;&nbsp;&nbsp;&nbsp;如果是log –&gt; flume –&gt; kafka<br>&nbsp;&nbsp;&nbsp;&nbsp;需要将设置为tail-F不能是-f：a1.sources.r1.command = tail -F /var/log/secure</p><h4 id="上传下载"><a href="#上传下载" class="headerlink" title="上传下载"></a>上传下载</h4><p>yum install -y lrzsz<br>sz xxx.log 下载 Linux–》window<br>rz 回车 上传</p><h4 id="alias：给命令取别名"><a href="#alias：给命令取别名" class="headerlink" title="alias：给命令取别名"></a>alias：给命令取别名</h4><p>生产上常用的命令或复杂的命令一串，可以使用别名来简化</p><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># alias gpr=<span class="hljs-string">&#x27;cd /root/practive&#x27;</span></span>[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># pwd</span>/root[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># gpr</span>[root<span class="hljs-symbol">@hadoop100</span> practive]<span class="hljs-meta"># pwd</span>/root/practive</code></pre><p>查看取过别名的命令</p><pre><code class="hljs vhdl">[root@hadoop100 practive]# <span class="hljs-keyword">alias</span> <span class="hljs-keyword">alias</span> cp=<span class="hljs-symbol">&#x27;cp</span> -i&#x27;<span class="hljs-keyword">alias</span> egrep=<span class="hljs-symbol">&#x27;egrep</span> <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> fgrep=<span class="hljs-symbol">&#x27;fgrep</span> <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> gpr=<span class="hljs-symbol">&#x27;cd</span> /root/practive&#x27;<span class="hljs-keyword">alias</span> grep=<span class="hljs-symbol">&#x27;grep</span> <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> l.=<span class="hljs-symbol">&#x27;ls</span> -d .* <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> ll=<span class="hljs-symbol">&#x27;ls</span> -l <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> ls=<span class="hljs-symbol">&#x27;ls</span> <span class="hljs-comment">--color=auto&#x27;</span><span class="hljs-keyword">alias</span> mv=<span class="hljs-symbol">&#x27;mv</span> -i&#x27;<span class="hljs-keyword">alias</span> rm=<span class="hljs-symbol">&#x27;rm</span> -i&#x27;</code></pre><p>现在新开一个会话</p><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> ~]<span class="hljs-meta"># gpr</span>bash: gpr: 未找到命令...</code></pre><p>原因是没有设置全局生效</p><h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><p>全局： /etc/profile 【一般权限不够，配置不了】<br>&nbsp;&nbsp;&nbsp;&nbsp;source /etc/profile，当前会话生效<br>&nbsp;&nbsp;&nbsp;&nbsp;已经开启的会话不会跟着自动生效，需要重新执行生效命令<br>&nbsp;&nbsp;&nbsp;&nbsp;新开的会话是自动的生效<br>个人： ~/.bash_profile、 ~/.bashrc  【推荐第二个】<br>&nbsp;&nbsp;<strong>【坑！！】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;如下场景：<br>&nbsp;&nbsp;&nbsp;&nbsp;ssh远程执行B机器 java命令抛错， java command not found<br>&nbsp;&nbsp;&nbsp;&nbsp;直接登录B机器which java找的到<br>&nbsp;&nbsp;&nbsp;&nbsp;原因：配置环境变量文件在 .bash_profile 是不正确的，应该配置在 .bashrc文件里。</p><h4 id="创建用户和设置密码"><a href="#创建用户和设置密码" class="headerlink" title="创建用户和设置密码"></a>创建用户和设置密码</h4><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 practive]</span># useradd lh<span class="hljs-string">[root@hadoop100 practive]</span># iduid=<span class="hljs-number">0</span>(root) gid=<span class="hljs-number">0</span>(root) 组=<span class="hljs-number">0</span>(root) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023<span class="hljs-string">[root@hadoop100 practive]</span># passwd lh更改用户 lh 的密码 。新的 密码：无效的密码： 密码未通过字典检查 - 过于简单化/系统化重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。</code></pre><h4 id="历史命令history"><a href="#历史命令history" class="headerlink" title="历史命令history"></a>历史命令history</h4><p>查看和清空历史命令</p><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop100</span> practive]<span class="hljs-meta"># history -c</span>[root<span class="hljs-symbol">@hadoop100</span> practive]<span class="hljs-meta"># history</span>    <span class="hljs-number">1</span>  history</code></pre><p>执行历史命令</p><pre><code class="hljs jboss-cli">[root@hadoop100 practive]<span class="hljs-comment"># history</span>    1  <span class="hljs-keyword">history</span>    2  <span class="hljs-keyword">pwd</span>    3  <span class="hljs-keyword">history</span>[root@hadoop100 practive]<span class="hljs-comment"># !2</span><span class="hljs-keyword">pwd</span><span class="hljs-string">/root/practive</span></code></pre><p>注：跳板机和直连服务器查不到删除历史命令的人<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;堡垒机可以查找到</p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>相应的坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux实用命令及注意事项3</title>
    <link href="/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B93/"/>
    <url>/2020/11/08/linux%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B93/</url>
    
    <content type="html"><![CDATA[<h3 id="欢迎来到onlyroads，希望我的分享能够对大家有所帮助"><a href="#欢迎来到onlyroads，希望我的分享能够对大家有所帮助" class="headerlink" title="欢迎来到onlyroads，希望我的分享能够对大家有所帮助"></a>欢迎来到onlyroads，希望我的分享能够对大家有所帮助</h3><hr><h4 id="磁盘"><a href="#磁盘" class="headerlink" title="磁盘"></a>磁盘</h4><p><strong>查看磁盘：df -h</strong></p><pre><code class="hljs angelscript"><span class="hljs-string">[[root@hadoop100 ~]</span># df -h文件系统                 容量  已用  可用 已用% 挂载点/dev/mapper/centos-root   <span class="hljs-number">46</span>G  <span class="hljs-number">8.5</span>G   <span class="hljs-number">37</span>G   <span class="hljs-number">19</span>% /devtmpfs                 <span class="hljs-number">2.0</span>G     <span class="hljs-number">0</span>  <span class="hljs-number">2.0</span>G    <span class="hljs-number">0</span>% /devtmpfs                    <span class="hljs-number">2.0</span>G     <span class="hljs-number">0</span>  <span class="hljs-number">2.0</span>G    <span class="hljs-number">0</span>% /dev/shmtmpfs                    <span class="hljs-number">2.0</span>G   <span class="hljs-number">13</span>M  <span class="hljs-number">2.0</span>G    <span class="hljs-number">1</span>% /runtmpfs                    <span class="hljs-number">2.0</span>G     <span class="hljs-number">0</span>  <span class="hljs-number">2.0</span>G    <span class="hljs-number">0</span>% /sys/fs/cgroup/dev/sda1               <span class="hljs-number">1014</span>M  <span class="hljs-number">157</span>M  <span class="hljs-number">858</span>M   <span class="hljs-number">16</span>% /boottmpfs                    <span class="hljs-number">394</span>M  <span class="hljs-number">8.0</span>K  <span class="hljs-number">394</span>M    <span class="hljs-number">1</span>% /run/user/<span class="hljs-number">42</span>tmpfs                    <span class="hljs-number">394</span>M     <span class="hljs-number">0</span>  <span class="hljs-number">394</span>M    <span class="hljs-number">0</span>% /run/user/<span class="hljs-number">0</span></code></pre><h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><pre><code class="hljs yaml">[<span class="hljs-string">root@hadoop100</span> <span class="hljs-string">~</span>]<span class="hljs-comment"># free -m</span>              <span class="hljs-string">total</span>        <span class="hljs-string">used</span>        <span class="hljs-string">free</span>      <span class="hljs-string">shared</span>  <span class="hljs-string">buff/cache</span>   <span class="hljs-string">available</span><span class="hljs-attr">Mem:</span>           <span class="hljs-number">3934         </span><span class="hljs-number">376</span>        <span class="hljs-number">3171          </span><span class="hljs-number">12</span>         <span class="hljs-number">385</span>        <span class="hljs-number">3267</span><span class="hljs-attr">Swap:</span>          <span class="hljs-number">3967           </span><span class="hljs-number">0</span>        <span class="hljs-number">3967</span></code></pre><p>预留内存最好在15%<br>376 / 3943 = 8%<br>swap 因为内存不够，使用部分磁盘空间来充当内存使用，虽然可以解决内存紧缺的问题，但是效率不高；尤其是大数据，swap哪怕设置了大小，也尽量设置惰性使用</p><h4 id="机器负载-top"><a href="#机器负载-top" class="headerlink" title="机器负载 top"></a>机器负载 top</h4><p>负载：load average: 0.00, 0.01, 0.04<br>                        1m   5m  15m<br>经验值：10  生产上尽量控制在10，否则服务器就认为很卡<br>a. 计算程序 hive sql、spark、flink 密集计算，要进行调优<br>b. 服务器被挖矿了（查看CPU是否占比很高）<br>                                                                    CPU%<br>      1 root      20   0  194080   7140   4180 S   0.0  0.2   0:02.81 systemd<br>      yarn 软件、redis 软件，默认端口号会被扫描 进行注入 挖矿<br>c. 硬件问题，内存条损坏，万能重启，检测是否是硬件问题</p><h4 id="安装卸载"><a href="#安装卸载" class="headerlink" title="安装卸载"></a>安装卸载</h4><p>yum install -y httpd<br>yum remove<br>公司内网 yum源<br>离线rpm包安装<br>yum install ./httpd.rpm</p><p>rpm -qa | grep httpd<br>rpm -e：清除 (卸载) 软件包<br>rpm –nodeps：不验证包依赖</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># rpm -aq | grep httpdhttpd-tools<span class="hljs-number">-2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos.x86_64httpd<span class="hljs-number">-2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos.x86_64<span class="hljs-string">[root@hadoop100 ~]</span># rpm -e httpd-tools<span class="hljs-number">-2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos.x86_64错误：依赖检测失败：httpd-tools = <span class="hljs-number">2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos 被 (已安裝) httpd<span class="hljs-number">-2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos.x86_64 需要<span class="hljs-string">[root@hadoop100 ~]</span># rpm -e --nodeps httpd-tools<span class="hljs-number">-2.4</span><span class="hljs-number">.6</span><span class="hljs-number">-95.</span>el7.centos.x86_64</code></pre><p>wget命令</p><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p>ps -ef | grep 进程名称</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># ps -ef | grep httpdroot       <span class="hljs-number">2137</span>      <span class="hljs-number">1</span>  <span class="hljs-number">7</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDapache     <span class="hljs-number">2139</span>   <span class="hljs-number">2137</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDapache     <span class="hljs-number">2141</span>   <span class="hljs-number">2137</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDapache     <span class="hljs-number">2142</span>   <span class="hljs-number">2137</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDapache     <span class="hljs-number">2143</span>   <span class="hljs-number">2137</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDapache     <span class="hljs-number">2144</span>   <span class="hljs-number">2137</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> ?        <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> /usr/sbin/httpd -DFOREGROUNDroot       <span class="hljs-number">2146</span>   <span class="hljs-number">2034</span>  <span class="hljs-number">0</span> <span class="hljs-number">22</span>:<span class="hljs-number">06</span> pts/<span class="hljs-number">0</span>    <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> grep --color=<span class="hljs-built_in">auto</span> httpd</code></pre><p>杀死进程<br>ps -ef | grep 名称<br>有可能匹配多个，仔细确认进程是否是自己想要杀的进程。<br>kill -9 pid 【高危命令】其他高危命令有：rm -rf，echo “” &gt; file<br>kill -9 111 112 113 三个进程一杀，会造成生成上的误杀事故<br>全局杀<br>kill -9 $(pgrep -f 匹配字符)</p><h4 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h4><p>netstat -nlp | grep pid/名称</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># netstat -nlp|grep httpdtcp6       <span class="hljs-number">0</span>      <span class="hljs-number">0</span> :::<span class="hljs-number">80</span>                   :::*                    LISTEN      <span class="hljs-number">2261</span>/httpd</code></pre><p>:::80、0.0.0.0: 80、192.168.1.100:80<br>80端口号可以对外，window或者其他服务器可以ip + 80进行访问<br>localhost、127.0.0.1代表本机<br>80端口仅本机额其他服务可以访问，window或者其他服务器无法访问</p><pre><code class="hljs awk">可以在<span class="hljs-regexp">/etc/</span>httpd<span class="hljs-regexp">/conf/</span>httpd.conf文件中修改[root@hadoop100 conf]<span class="hljs-comment"># pwd</span><span class="hljs-regexp">/etc/</span>httpd/conf[root@hadoop100 conf]<span class="hljs-comment"># vim httpd.conf</span><span class="hljs-comment">#Listen 12.34.56.78:80</span>Listen <span class="hljs-number">80</span></code></pre><p>总结：<br>a. 有进程PID不一定就有端口号<br>b. 服务的通信交流，其实就是要 ip + 端口号</p><pre><code class="hljs routeros">[root@hadoop100 ~]# telnet 192.169.1.100 221Trying 192.169.1.100<span class="hljs-built_in">..</span>.telnet: connect <span class="hljs-keyword">to</span><span class="hljs-built_in"> address </span>192.169.1.100:<span class="hljs-built_in"> Connection </span>refused</code></pre><p>Connection refused 【错误三】 command not found【错误一】 permission denied【错误二】<br>linux机器去访问 服务器的服务<br>    先 ping ip<br>    再 telnet ip port</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 ~]</span># telnet hadoop100 <span class="hljs-number">22</span>Trying <span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.100</span>...Connected to hadoop100.Escape character <span class="hljs-keyword">is</span> <span class="hljs-string">&#x27;^]&#x27;</span>.SSH<span class="hljs-number">-2.0</span>-OpenSSH_7<span class="hljs-number">.4</span></code></pre><p>windows电脑去访问 服务器的服务<br>    先 ping ip<br>    再 telnet ip port<br><strong>注意</strong><br>今后如果ping ip可以通，但是telnet不通，但是ip和端口号都开了<br>很有可能是端口号对应的是localhost/127.0.0.1，外网访问不了</p><h4 id="压缩、解压"><a href="#压缩、解压" class="headerlink" title="压缩、解压"></a>压缩、解压</h4><p><strong>zip</strong><br>压缩</p><pre><code class="hljs yaml">[<span class="hljs-string">root@hadoop100</span> <span class="hljs-string">practive</span>]<span class="hljs-comment"># zip -r ruozedata.zip ruozedata/*</span>  <span class="hljs-attr">adding:</span> <span class="hljs-string">ruozedata/1.txt</span> <span class="hljs-string">(stored</span> <span class="hljs-number">0</span><span class="hljs-string">%)</span>[<span class="hljs-string">root@hadoop100</span> <span class="hljs-string">practive</span>]<span class="hljs-comment"># ll</span><span class="hljs-string">drwxr-xr-x.</span> <span class="hljs-number">2</span> <span class="hljs-string">root</span> <span class="hljs-string">root</span>  <span class="hljs-number">19</span> <span class="hljs-number">11</span><span class="hljs-string">月</span> <span class="hljs-number">13</span> <span class="hljs-number">22</span><span class="hljs-string">:48</span> <span class="hljs-string">ruozedata</span><span class="hljs-string">-rw-r--r--.</span> <span class="hljs-number">1</span> <span class="hljs-string">root</span> <span class="hljs-string">root</span> <span class="hljs-number">198</span> <span class="hljs-number">11</span><span class="hljs-string">月</span> <span class="hljs-number">13</span> <span class="hljs-number">22</span><span class="hljs-string">:49</span> <span class="hljs-string">ruozedata.zip</span></code></pre><p>解压</p><pre><code class="hljs angelscript"><span class="hljs-string">[root@hadoop100 practive]</span># rm -rf ruozedata<span class="hljs-string">[root@hadoop100 practive]</span># unzip ruozedata.zip Archive:  ruozedata.zip extracting: ruozedata/<span class="hljs-number">1.</span>txt         <span class="hljs-string">[root@hadoop100 practive]</span># lldrwxr-xr-x. <span class="hljs-number">2</span> root root  <span class="hljs-number">19</span> <span class="hljs-number">11</span>月 <span class="hljs-number">13</span> <span class="hljs-number">22</span>:<span class="hljs-number">50</span> ruozedata-rw-r--r--. <span class="hljs-number">1</span> root root <span class="hljs-number">198</span> <span class="hljs-number">11</span>月 <span class="hljs-number">13</span> <span class="hljs-number">22</span>:<span class="hljs-number">49</span> ruozedata.zip</code></pre><p><strong>tar</strong><br>压缩：tar -zcvf 文件<br>解压：tar -zxvf 文件</p><h4 id="commond-not-fount"><a href="#commond-not-fount" class="headerlink" title="commond not fount"></a>commond not fount</h4><p>原因：<br>    没有安装/没有配置环境变量<br>    配置环境变量：<br>    export JAVA_HOME=/usr/java/jdk1.8<br>    export PATH=$JAVA_HOME/bin:$PATH  推荐/bin目录应该配置在PATH前面<br>    这样当系统查找命令时，会先从配置的环境变量中查找，一旦找到PATH后面的内容就不再进行查找了</p><h4 id="定时"><a href="#定时" class="headerlink" title="定时"></a>定时</h4><p>crontab -l 查看<br>crontab -e 编辑 就是编辑一个定时器文件内容</p><p>面试题：<br>每隔10s 打印一次（contab无法完成，最小是分）<br>*/6 * * * *  每隔6min打印，是错误的</p><p>分<br>小时<br>日<br>月<br>周<br>标识 每</p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>相应的坑</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
